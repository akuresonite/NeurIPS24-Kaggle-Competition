{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca027266-e09d-4d56-8456-36ff0fb3842f",
   "metadata": {},
   "source": [
    "## Model Creation on Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330bf9f2-a23a-411b-9dd3-3843f2f8d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 295246830\n",
    "one_len = 1589906\n",
    "zero_len = 293656924\n",
    "protein_map = {'BRD4': 1, 'HSA': 2, 'sEH': 3}\n",
    "vocab = {'C': 6825082866, '#': 81527490, '@': 511451694, 'H': 456489972, '=': 1406606874, 'O': 2554179786,\n",
    "         'N': 2469595230, 'c': 12257477022, '-': 438483636, '.': 216945504, 'l': 491088828, 'B': 123330132,\n",
    "         'r': 121915914, 'n': 1997759694, 'D': 295246830, 'y': 295246830, 'o': 67918650, 's': 156618468,\n",
    "         'S': 90662574, 'F': 492710238, '+': 65206260, 'i': 1414026, '/': 11547096, 'I': 23972994}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import LongType, IntegerType, StructType, StructField\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7f7028-db0d-48ef-9902-8561b5902eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a564012-333f-4061-a649-f8814b1d3c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/27 00:45:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/27 00:45:47 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://passpoli.ieor.iitb.ac.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[64]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>leash belka3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5a19f26720>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 256 Gb and 64 Cores\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka3\")\n",
    "    .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "    .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "    .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "    .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "    # .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "    .config(\"spark.local.dir\", \"/scratch/23m1521/temp\")  # Specify a directory with enough space\n",
    "    .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "    .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "    .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n",
    "\n",
    "# SynapseML \n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.8\")\n",
    "#     .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"64g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"64g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"8\")  # Reduced number of executors\n",
    "#     .config(\"spark.executor.cores\", \"8\")  # Increased cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.7\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"1000\")  # Increase shuffle partitions\n",
    "#     .config(\"spark.ui.enabled\", \"true\")  # Enable Spark UI\n",
    "#     .master(\"local[8]\")  # Reduced number of cores for local mode\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc05b7f8-87f5-495d-a67d-f60e8b7ac823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df0_features = spark.read.format('parquet').load('zero_features.parquet')\n",
    "df1_features = spark.read.format('parquet').load('one_features.parquet')\n",
    "\n",
    "full_df = df0_features.union(df1_features).orderBy(F.rand())\n",
    "\n",
    "# print(df0_features.rdd.getNumPartitions())\n",
    "# print(full_df.count())\n",
    "# df0_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d63a68-9b90-4a27-954c-4a5f0043db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df = full_df.sample(fraction=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25184b3-bb4a-4ed3-b17c-8bbd1c8b72b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "protein_ohe = OneHotEncoder(inputCol=\"protein\", outputCol=\"protein_onehot\")\n",
    "protein_ohe = protein_ohe.fit(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6124f07c-0615-4b2c-bfbc-99d7ee47df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = protein_ohe.transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752f135f-caca-4e7d-b041-616739fa68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = full_df.columns[-1:] + full_df.columns[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc838420-81eb-466e-aa9e-50e934da493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=features_cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45eb6b11-45df-4721-9a7b-769263f3de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df2 = vectorAssembler.transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947127dc-3047-4e3c-a155-ce4ae137571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(full_df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab00b080-dbdf-453f-ab26-479c4b5a5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df2 = full_df2.repartition(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f4ae2-5c06-4df1-9612-e1be86a55767",
   "metadata": {},
   "source": [
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a3e89b-3f6c-483a-8423-2e5580838e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/horovod/torch/mpi_lib_v2.cpython-312-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still available.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from packaging import version\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import horovod.spark.torch as hvd\n",
    "from horovod.spark.common.backend import SparkBackend\n",
    "from horovod.spark.common.store import Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4866d0-e96d-473a-a953-751018e56a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfbbf6c5-8416-467c-8a41-e9a79bc26777",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = full_df2.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2328b625-ab7f-4e1c-a892-0f43c5adbc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Store.create('/scratch/23m1521/temp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e503c7c-db0f-48a7-9bf4-c7a7305dec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 99\n",
    "model = BinaryClassifier(input_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f81410be-d48d-4181-ac7d-a3ecc7febb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchEstimator_6040fd634650"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Horovod Spark Estimator on the DataFrame\n",
    "backend = SparkBackend(num_proc=spark.sparkContext.defaultParallelism,\n",
    "                       stdout=sys.stdout, stderr=sys.stderr,\n",
    "                       prefix_output_with_timestamp=True)\n",
    "\n",
    "torch_estimator = hvd.TorchEstimator(backend=backend,\n",
    "                                     store=store,\n",
    "                                     model=model,\n",
    "                                     optimizer=optimizer,\n",
    "                                     loss=lambda input, target: criterion(input, target.long()),\n",
    "                                     input_shapes=[[-1, 99]],\n",
    "                                     feature_cols=['features'],\n",
    "                                     label_cols=['y'],\n",
    "                                     batch_size=256,\n",
    "                                     epochs=10,\n",
    "                                     validation=0.1,\n",
    "                                     backward_passes_per_step=1,\n",
    "                                     verbose=1,\n",
    "                                     use_gpu=True\n",
    "                                    )\n",
    "torch_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89f09f5c-6ff7-406c-a6d9-ca10b85c6906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_partitions=640\n",
      "writing dataframes\n",
      "train_data_path=file:///scratch/23m1521/temp2/intermediate_train_data.0\n",
      "val_data_path=file:///scratch/23m1521/temp2/intermediate_val_data.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_partitions=576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                      (0 + 64) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[571.352s][warning][gc,alloc] Executor task launch worker for task 13.0 in stage 12.0 (TID 549): Retried waiting for GCLocker too often allocating 4194304 words\n",
      "[571.358s][warning][gc,alloc] Executor task launch worker for task 57.0 in stage 12.0 (TID 593): Retried waiting for GCLocker too often allocating 4194304 words\n",
      "[571.358s][warning][gc,alloc] Executor task launch worker for task 33.0 in stage 12.0 (TID 569): Retried waiting for GCLocker too often allocating 4194304 words\n",
      "[571.358s][warning][gc,alloc] Executor task launch worker for task 54.0 in stage 12.0 (TID 590): Retried waiting for GCLocker too often allocating 325781 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/27 00:55:02 WARN TaskMemoryManager: Failed to allocate a page (33554416 bytes), try again.\n",
      "24/12/27 00:55:02 WARN TaskMemoryManager: Failed to allocate a page (33554416 bytes), try again.\n",
      "24/12/27 00:55:02 WARN TaskMemoryManager: Failed to allocate a page (33554416 bytes), try again.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_partitions=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'pyarrow.lib.Schema' object has no attribute 'to_arrow_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msetOutputCols([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/horovod/spark/common/estimator.py:35\u001b[0m, in \u001b[0;36mHorovodEstimator.fit\u001b[0;34m(self, df, params)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model to the DataFrame.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m        `HorovodModel` transformer wrapping the trained model.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHorovodEstimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/horovod/spark/common/estimator.py:68\u001b[0m, in \u001b[0;36mHorovodEstimator._fit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[1;32m     67\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create_backend()\n\u001b[0;32m---> 68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetLabelCols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetFeatureCols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetValidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msample_weight_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetSampleWeightCol\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcompress_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetCompressSparseCols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitions_per_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetPartitionsPerProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetVerbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_row_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_metadata_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/horovod/spark/common/util.py:735\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(num_processes, store, df, label_columns, feature_columns, validation, sample_weight_col, compress_sparse, partitions_per_process, verbose)\u001b[0m\n\u001b[1;32m    733\u001b[0m key \u001b[38;5;241m=\u001b[39m _training_cache\u001b[38;5;241m.\u001b[39mcreate_key(df, store, validation)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _training_cache\u001b[38;5;241m.\u001b[39muse_key(key):\n\u001b[0;32m--> 735\u001b[0m     dataset_idx \u001b[38;5;241m=\u001b[39m \u001b[43m_get_or_create_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m dataset_idx\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/horovod/spark/common/util.py:672\u001b[0m, in \u001b[0;36m_get_or_create_dataset\u001b[0;34m(key, store, df, feature_columns, label_columns, validation, sample_weight_col, compress_sparse, num_partitions, num_processes, verbose)\u001b[0m\n\u001b[1;32m    667\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeout while waiting for all parquet-store files to appear, Please \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    668\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheck whether these files were saved successfully when materializing \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    669\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe. Internal Error: \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[0;32m--> 672\u001b[0m train_rows, val_rows, pq_metadata, avg_row_size \u001b[38;5;241m=\u001b[39m \u001b[43mget_simple_meta_from_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_rows=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_rows))\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/horovod/spark/common/util.py:495\u001b[0m, in \u001b[0;36mget_simple_meta_from_parquet\u001b[0;34m(store, label_columns, feature_columns, sample_weight_col, dataset_idx)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ex)\n\u001b[1;32m    494\u001b[0m train_data \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_parquet_dataset(train_data_path)\n\u001b[0;32m--> 495\u001b[0m train_data_schema \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arrow_schema\u001b[49m()\n\u001b[1;32m    496\u001b[0m train_rows, train_data_total_byte_size \u001b[38;5;241m=\u001b[39m _get_dataset_info(train_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    497\u001b[0m                                                            train_data_path)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# Write train metadata to filesystem\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'pyarrow.lib.Schema' object has no attribute 'to_arrow_schema'"
     ]
    }
   ],
   "source": [
    "torch_model = torch_estimator.fit(train_df).setOutputCols(['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c06bf-738f-47df-9d73-bd6e28a5c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/27 02:03:40 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /scratch/23m1521/temp/blockmgr-b2a91130-8c98-4f76-a6f3-29c11d4eb0b1. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /scratch/23m1521/temp/blockmgr-b2a91130-8c98-4f76-a6f3-29c11d4eb0b1\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "pred_df = torch_model.transform(test_df)\n",
    "\n",
    "argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\n",
    "evaluator = MulticlassClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderPR')\n",
    "print('Test accuracy:', evaluator.evaluate(pred_df))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d5974-2fb1-4080-88b4-8c2c411615aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c30fc485-4316-4f61-b6b7-83e4fad4cc7d",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
