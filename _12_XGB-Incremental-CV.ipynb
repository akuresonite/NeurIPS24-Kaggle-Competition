{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_len = 295246830\n",
    "one_len = 1589906\n",
    "zero_len = 293656924\n",
    "test_len = 1674896\n",
    "protein_map = {'BRD4': 1, 'HSA': 2, 'sEH': 3}\n",
    "vocab = {'C': 6825082866, '#': 81527490, '@': 511451694, 'H': 456489972, '=': 1406606874, 'O': 2554179786,\n",
    "         'N': 2469595230, 'c': 12257477022, '-': 438483636, '.': 216945504, 'l': 491088828, 'B': 123330132,\n",
    "         'r': 121915914, 'n': 1997759694, 'D': 295246830, 'y': 295246830, 'o': 67918650, 's': 156618468,\n",
    "         'S': 90662574, 'F': 492710238, '+': 65206260, 'i': 1414026, '/': 11547096, 'I': 23972994}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, IntegerType, StructType, StructField, ArrayType, DoubleType, StringType\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StringIndexerModel, OneHotEncoderModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from functools import wraps\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import Parallel, delayed\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski, rdmolops, AllChem, rdchem, rdEHTTools, rdMolDescriptors\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from padelpy import from_smiles\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/04 19:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/04 19:11:25 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://passpoli.ieor.iitb.ac.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>leash belka3467</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8783ed3ec0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # for 256 Gb and 64 Cores\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     # .config(\"spark.local.dir\", \"/scratch/23m1521/temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka3467\")\n",
    "    .config(\"spark.driver.memory\", \"64g\")  # Increased driver memory for large jobs\n",
    "    .config(\"spark.executor.memory\", \"64g\")  # Increased executor memory\n",
    "    .config(\"spark.executor.instances\", \"32\")  # 32 executors\n",
    "    .config(\"spark.executor.cores\", \"2\")  # 2 cores per executor\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\")  # Driver result size limit\n",
    "    .config(\"spark.local.dir\", \"temp\")  # Ensure high-speed storage\n",
    "    .config(\"spark.shuffle.file.buffer\", \"1024k\")  # Larger shuffle buffer for better IO\n",
    "    .config(\"spark.memory.fraction\", \"0.85\")  # Increased memory for tasks\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.7\")  # Increased shuffle memory\n",
    "    .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "    .master(\"local[*]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark\n",
    "\n",
    "\n",
    "# # GPU\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3467\")\n",
    "#     .config(\"spark.driver.memory\", \"64g\")\n",
    "#     .config(\"spark.executor.memory\", \"64g\")\n",
    "#     .config(\"spark.executor.instances\", \"2\")  # 2 executors, one per GPU\n",
    "#     .config(\"spark.executor.cores\", \"32\")  # Divide cores equally between executors (64/2)\n",
    "#     .config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "#     .config(\"spark.local.dir\", \"temp\")\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"1024k\")\n",
    "#     .config(\"spark.memory.fraction\", \"0.85\")\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.7\")\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx64g\")\n",
    "#     .config(\"spark.executor.resource.gpu.amount\", \"1\") # Assign 1 GPU per executor\n",
    "#     .config(\"spark.master\", \"local[*]\") # Important: Use local cluster mode to enable GPU scheduling\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "# spark\n",
    "\n",
    "# SparkSession for 128 GB RAM and 64 cores\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"Optimized Spark for 128GB RAM and 64 Cores\")\n",
    "#     .config(\"spark.driver.memory\", \"64g\")  # 64GB for driver memory\n",
    "#     .config(\"spark.executor.memory\", \"64g\")  # 64GB for executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor (total = 64 cores)\n",
    "#     .config(\"spark.driver.maxResultSize\", \"8g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Temp directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"512k\")  # Increased shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# SynapseML \n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.8\")\n",
    "#     .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"64g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"64g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"8\")  # Reduced number of executors\n",
    "#     .config(\"spark.executor.cores\", \"8\")  # Increased cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.7\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"1000\")  # Increase shuffle partitions\n",
    "#     .config(\"spark.ui.enabled\", \"true\")  # Enable Spark UI\n",
    "#     .master(\"local[8]\")  # Reduced number of cores for local mode\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "datadir = \"/home/23m1521/ashish/kaggle/full_feat_tok_df_vectors.parquet\"\n",
    "chunks_path = sorted([os.path.join(datadir, i) for i in os.listdir(datadir) if i.endswith(\".parquet\")])\n",
    "total_chunks = len(chunks_path)\n",
    "print(total_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_chunk(path):\n",
    "    return spark.read.format('parquet').load(path)\n",
    "\n",
    "def add_sample_weights(df):\n",
    "    class_counts = df.groupBy(\"binds\").count().collect()\n",
    "    total_count = sum(row[\"count\"] for row in class_counts)\n",
    "    class_weights = {row[\"binds\"]: total_count / (2 * row[\"count\"]) for row in class_counts}\n",
    "    return df.withColumn(\"sample_weights\", when(col(\"binds\") == 0, class_weights[0]).when(col(\"binds\") == 1, class_weights[1]))\n",
    "\n",
    "def get_scale_pos_weight(df):\n",
    "    class_counts = dict(df.groupBy(\"binds\").count().collect())\n",
    "    return class_counts[0]/class_counts[1]\n",
    "    \n",
    "def make_dataset(df, chunk_df_count):\n",
    "    def process_row(row):\n",
    "        return (row['vectors'].toArray(), row['binds'], row['sample_weights'])\n",
    "    features, labels, weights = [], [], []\n",
    "    for feature, label, weight in tqdm(df.rdd.map(process_row).toLocalIterator(), total=chunk_df_count):\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "    return features, labels, weights\n",
    "\n",
    "def make_dataset2(df, test=False):\n",
    "    df = df.toPandas()\n",
    "    df.vectors = df.vectors.map(lambda x: x.toArray())\n",
    "    if test == True:\n",
    "        return df.id.values, np.array([i for i in df.vectors.values]), None, None\n",
    "    return df.id.values, np.array([i for i in df.vectors.values]), df.binds.values, df.sample_weights.values\n",
    "\n",
    "def save_checkpoint(model, params, i, evals_result,  path, save_name):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    model.save_model(os.path.join(path, f\"{save_name}.json\"))\n",
    "    joblib.dump({\"params\": params, 'i': i, \"evals_result\": evals_result}, os.path.join(path, f\"{save_name}_params.joblib\"))\n",
    "    print(\"Model saved at\", path)\n",
    "\n",
    "def load_checkpoint(path, save_name):\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(os.path.join(path, f\"{save_name}.json\"))\n",
    "    ckpt = joblib.load(os.path.join(path, f\"{save_name}_params.joblib\"))\n",
    "    params, i = ckpt['params'], ckpt['i']\n",
    "    print(\"Model loaded from\", path)\n",
    "    return model, params, i\n",
    "\n",
    "def train_xgb(dmatrix, xgb_model=None):\n",
    "    lr = [0.1, 0.07, 0.04, 0.01, 0.007]\n",
    "    best_params1 = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'aucpr'],\n",
    "    'subsample': 1.0, \n",
    "    'rate_drop': 0.4,\n",
    "    'skip_drop': 0.5,\n",
    "    'min_child_weight': 3, \n",
    "    'max_depth': 3, \n",
    "    'lambda': 5, \n",
    "    'gamma': 0, \n",
    "    'eta': lr[0], \n",
    "    'seed': 42,\n",
    "    'colsample_bytree': 1.0, \n",
    "    'alpha': 5,\n",
    "    'device': 'cuda',\n",
    "    # 'device': 'cpu'\n",
    "    }\n",
    "    evals_result = {'train': {'logloss': [], 'aucpr': []}}\n",
    "    \n",
    "    bst = xgb.train(\n",
    "        best_params1, \n",
    "        dmatrix, \n",
    "        num_boost_round=100,\n",
    "        evals=[(dmatrix, 'train')], \n",
    "        evals_result=evals_result, \n",
    "        verbose_eval=False,\n",
    "        xgb_model=xgb_model\n",
    "        # early_stopping_rounds=500,\n",
    "        )\n",
    "    return bst, evals_result, best_params1\n",
    "\n",
    "def delete_df_chunk(df):\n",
    "    df.unpersist()\n",
    "    del df\n",
    "\n",
    "def spark_suppress_logs(level=\"ERROR\", reset_level=\"INFO\"):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            spark.sparkContext.setLogLevel(level)\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                spark.sparkContext.setLogLevel(reset_level)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@spark_suppress_logs()\n",
    "def incrementally_train():\n",
    "    with tqdm(total=total_chunks, dynamic_ncols=True) as pbar1:\n",
    "        sample_count = 0\n",
    "        xgb_model = None\n",
    "        ckpt_dir = \"Incrementally_train_XGB_ckpt\"\n",
    "        \n",
    "        for i, chunk_path in enumerate(chunks_path):\n",
    "            # if i > 1:\n",
    "            #     break\n",
    "        \n",
    "            pbar1.set_description(f\"Chunk: {i+1}\")\n",
    "            \n",
    "            \n",
    "            # --- Load chunk --------------------------------------\n",
    "            chunk_df = load_df_chunk(chunk_path)\n",
    "            chunk_df = add_sample_weights(chunk_df)\n",
    "            chunk_df = chunk_df.repartition(1)\n",
    "            chunk_df_count = chunk_df.count()\n",
    "            \n",
    "            # --- Getting Dataset ---------------------------------\n",
    "            _, features, labels, weights = make_dataset2(chunk_df)\n",
    "            dtrain = xgb.DMatrix(data=features, label=labels, weight=weights, nthread=25)\n",
    "            \n",
    "            # # --- Train ------------------------------------------\n",
    "            if xgb_model is None:\n",
    "                xgb_model, evals_result, params = train_xgb(dtrain)\n",
    "            else:\n",
    "                xgb_model, evals_result, params = train_xgb(dtrain, xgb_model)\n",
    "            save_checkpoint(xgb_model, params, i, evals_result, ckpt_dir, f\"_{i+1}_ckpt\")\n",
    "            \n",
    "            # # --- Model Evaluation -------------------------------\n",
    "            loss = np.mean(evals_result['train']['logloss'])\n",
    "            aucpr = np.mean(evals_result['train']['aucpr'])\n",
    "            print(f\"Chunk {i+1} trained. Loss: {loss}, AUCPR: {aucpr}\")\n",
    "            \n",
    "            \n",
    "            # --- Clean up ----------------------------------------\n",
    "            # delete_df_chunk(chunk_df)\n",
    "            del chunk_df, features, labels, weights, dtrain, _\n",
    "            sample_used = sample_count + chunk_df_count\n",
    "            sample_used_percentage = (sample_used / train_len) * 100\n",
    "            remaining_samples = train_len - sample_used\n",
    "            remaining_samples_percentage = (remaining_samples / train_len) * 100\n",
    "            pbar1.set_postfix_str(\n",
    "                f\"{sample_used} ({sample_used_percentage:.2f}%) samples used,\" \n",
    "                f\"{remaining_samples} ({remaining_samples_percentage:.2f}%) samples remaining\"\n",
    "            )\n",
    "            pbar1.update(1)\n",
    "            sample_count += chunk_df_count\n",
    "            \n",
    "        # --- Save final model -----------------------------------\n",
    "        save_checkpoint(xgb_model, params, i, evals_result, ckpt_dir, f\"Final_ckpt\")\n",
    "\n",
    "\n",
    "@spark_suppress_logs()\n",
    "def incrementally_inference(model, chunks_path, save_dir, test=False):\n",
    "    with tqdm(total=total_chunks, dynamic_ncols=True) as pbar1:\n",
    "        sample_count = 0\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # id_array = np.array([], dtype=np.uint64)\n",
    "        # preds_array = np.array([], dtype=np.float128)\n",
    "        id_array = np.array([])\n",
    "        preds_array = np.array([])\n",
    "        \n",
    "        for i, chunk_path in enumerate(chunks_path):\n",
    "            pbar1.set_description(f\"Chunk: {i+1}\")\n",
    "            \n",
    "            \n",
    "            # --- Load chunk --------------------------------------\n",
    "            chunk_df = load_df_chunk(chunk_path)\n",
    "            # chunk_df = add_sample_weights(chunk_df)\n",
    "            chunk_df = chunk_df.repartition(1)\n",
    "            chunk_df_count = chunk_df.count()\n",
    "            \n",
    "            # --- Getting Dataset ---------------------------------\n",
    "            ids, features, labels, weights = make_dataset2(chunk_df, test=True)\n",
    "            dtest = xgb.DMatrix(data=features, nthread=25)\n",
    "            \n",
    "            # # --- Predict ---------------------------------------\n",
    "            preds = model.predict(dtest)\n",
    "\n",
    "            preds_array = np.concatenate([preds_array, preds])\n",
    "            id_array = np.concatenate([id_array, ids])\n",
    "            \n",
    "            # --- Save predictions -------------------------------\n",
    "            chunk_save_path = os.path.join(save_dir, f\"chunk_{i + 1}.npz\")\n",
    "            if test:\n",
    "                np.savez(chunk_save_path, ids=ids, preds=preds)\n",
    "            else:\n",
    "                np.savez(chunk_save_path, ids=ids, labels=labels, preds=preds)\n",
    "    \n",
    "            \n",
    "            # --- Clean up ----------------------------------------\n",
    "            del chunk_df, features, labels, weights, dtest, ids, preds\n",
    "            sample_used = sample_count + chunk_df_count\n",
    "            sample_used_percentage = (sample_used / train_len) * 100\n",
    "            remaining_samples = train_len - sample_used\n",
    "            remaining_samples_percentage = (remaining_samples / train_len) * 100\n",
    "            pbar1.set_postfix_str(\n",
    "                f\"{sample_used} ({sample_used_percentage:.2f}%) samples used,\" \n",
    "                f\"{remaining_samples} ({remaining_samples_percentage:.2f}%) samples remaining\"\n",
    "            )\n",
    "            pbar1.update(1)\n",
    "            sample_count += chunk_df_count\n",
    "            \n",
    "    if test:\n",
    "        return {'id': id_array, 'binds': preds_array}\n",
    "\n",
    "\n",
    "def load_npzs(d):\n",
    "    id_array = np.array([], dtype=np.uint64)\n",
    "    preds_array = np.array([], dtype=np.float128)\n",
    "    if os.path.exists(d):\n",
    "        for f in os.listdir(d):\n",
    "            if f.endswith(\".npz\"):\n",
    "                try:\n",
    "                    with np.load(os.path.join(d, f)) as data:\n",
    "                        preds_array = np.concatenate([preds_array, data[\"preds\"]])\n",
    "                        id_array = np.concatenate([id_array, data[\"ids\"]])\n",
    "                except: pass\n",
    "    return id_array, preds_array\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "def stratified_split(df, stratify_col, train_ratio=0.8, seed=None):\n",
    "    if stratify_col not in df.columns:\n",
    "        print(f\"Stratify column '{stratify_col}' not found in DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    train_df = None\n",
    "    test_df = None\n",
    "\n",
    "    for value in df.select(stratify_col).distinct().collect():\n",
    "        value = value[0]  # Extract the actual value from the Row object\n",
    "        \n",
    "        # Sample a fraction of the data for the current value\n",
    "        sampled_df = df.filter(col(stratify_col) == value).sample(withReplacement=False, fraction=train_ratio, seed=seed)\n",
    "        \n",
    "        # Create the test set by excluding the sampled data\n",
    "        remaining_df = df.filter(~col(\"id\").isin([row.id for row in sampled_df.select(\"id\").collect()]) if \"id\" in df.columns else ~df.isin(sampled_df))\n",
    "\n",
    "        if train_df is None:\n",
    "            train_df = sampled_df\n",
    "            test_df = remaining_df\n",
    "        else:\n",
    "            train_df = train_df.union(sampled_df)\n",
    "            test_df = test_df.union(remaining_df)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# incrementally_train()\n",
    "# spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Load chunk --------------------------------------\n",
    "# chunk_df = load_df_chunk(chunks_path[1])\n",
    "# train_df, test_df = chunk_df.randomSplit([0.8, 0.2], seed=42)\n",
    "# train_df, test_df = add_sample_weights(train_df), add_sample_weights(test_df)\n",
    "# train_df, test_df = train_df.repartition(1), test_df.repartition(1)\n",
    "# train_df_count, test_df_count = train_df.count(), test_df.count()\n",
    "# print(train_df_count, test_df_count)\n",
    "\n",
    "# class_counts = dict(chunk_df.groupBy(\"binds\").count().collect())\n",
    "# scale_pos_weight = class_counts[0]/class_counts[1]\n",
    "# print(scale_pos_weight, class_counts)\n",
    "\n",
    "# # --- Getting Dataset ---------------------------------\n",
    "# train_ids, train_features, train_labels, train_weights = make_dataset2(train_df)\n",
    "# test_ids, test_features, test_labels, test_weights = make_dataset2(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476728\n",
      "186.8549802824068 {0: 1468867, 1: 7861}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# --- Load chunk --------------------------------------\n",
    "chunk_df = load_df_chunk(chunks_path[1])\n",
    "chunk_df = add_sample_weights(chunk_df)\n",
    "chunk_df = chunk_df.repartition(1)\n",
    "chunk_df_count = chunk_df.count()\n",
    "print(chunk_df_count)\n",
    "\n",
    "class_counts = dict(chunk_df.groupBy(\"binds\").count().collect())\n",
    "scale_pos_weight = class_counts[0]/class_counts[1]\n",
    "print(scale_pos_weight, class_counts)\n",
    "\n",
    "# --- Getting Dataset ---------------------------------\n",
    "ids, features, labels, weights = make_dataset2(chunk_df)\n",
    "# dtrain = xgb.DMatrix(data=features, label=labels, weight=weights, nthread=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XGB_HPC'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STUDY_NAME = f\"XGB_HPC\"\n",
    "STUDY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed '/home/23m1521/ashish/kaggle/db_XGB_HPC.sqlite3'\n"
     ]
    }
   ],
   "source": [
    "!rm -vrf \"/home/23m1521/ashish/kaggle/db_XGB_HPC.sqlite3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 06:00:20,387] A new study created in RDB with name: XGB_HPC\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda94a1d6fc0471690ce823100abb8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 06:09:11,185] Trial 5 finished with value: -0.26471143980647943 and parameters: {'max_depth': 5, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.9, 'gamma': 1.0, 'min_child_weight': 5, 'lambda': 5, 'alpha': 5, 'n_estimators': 500}. Best is trial 5 with value: -0.26471143980647943.\n",
      "[I 2025-01-03 06:11:07,183] Trial 8 finished with value: -0.35034939755464173 and parameters: {'max_depth': 13, 'eta': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 5, 'lambda': 10, 'alpha': 1, 'n_estimators': 100}. Best is trial 8 with value: -0.35034939755464173.\n",
      "[I 2025-01-03 06:17:13,610] Trial 1 finished with value: -0.33134229471177273 and parameters: {'max_depth': 5, 'eta': 0.5, 'subsample': 0.9, 'colsample_bytree': 1.0, 'gamma': 1.0, 'min_child_weight': 1, 'lambda': 0, 'alpha': 10, 'n_estimators': 1000}. Best is trial 8 with value: -0.35034939755464173.\n",
      "[I 2025-01-03 06:17:48,087] Trial 9 finished with value: -0.3145836479253691 and parameters: {'max_depth': 5, 'eta': 0.05, 'subsample': 0.5, 'colsample_bytree': 0.6, 'gamma': 1.0, 'min_child_weight': 5, 'lambda': 10, 'alpha': 0, 'n_estimators': 1000}. Best is trial 8 with value: -0.35034939755464173.\n",
      "[I 2025-01-03 06:22:55,099] Trial 12 finished with value: -0.34004800736064356 and parameters: {'max_depth': 11, 'eta': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.6, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 0, 'alpha': 10, 'n_estimators': 100}. Best is trial 8 with value: -0.35034939755464173.\n",
      "[I 2025-01-03 06:23:44,860] Trial 14 finished with value: -0.19623986169396293 and parameters: {'max_depth': 3, 'eta': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.6, 'gamma': 1.0, 'min_child_weight': 3, 'lambda': 0, 'alpha': 1, 'n_estimators': 50}. Best is trial 8 with value: -0.35034939755464173.\n",
      "[I 2025-01-03 06:24:31,873] Trial 0 finished with value: -0.35580943834780965 and parameters: {'max_depth': 7, 'eta': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.6, 'gamma': 0.1, 'min_child_weight': 7, 'lambda': 10, 'alpha': 5, 'n_estimators': 1000}. Best is trial 0 with value: -0.35580943834780965.\n",
      "[I 2025-01-03 06:24:45,818] Trial 15 finished with value: -0.25520109415863035 and parameters: {'max_depth': 5, 'eta': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.5, 'gamma': 1.0, 'min_child_weight': 3, 'lambda': 10, 'alpha': 5, 'n_estimators': 50}. Best is trial 0 with value: -0.35580943834780965.\n",
      "[I 2025-01-03 06:26:22,416] Trial 16 finished with value: -0.2501858346401085 and parameters: {'max_depth': 9, 'eta': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 7, 'lambda': 1, 'alpha': 0, 'n_estimators': 50}. Best is trial 0 with value: -0.35580943834780965.\n",
      "[I 2025-01-03 06:29:19,771] Trial 18 finished with value: -0.2957673417706147 and parameters: {'max_depth': 11, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.6, 'gamma': 0.5, 'min_child_weight': 5, 'lambda': 1, 'alpha': 5, 'n_estimators': 50}. Best is trial 0 with value: -0.35580943834780965.\n",
      "[I 2025-01-03 06:38:11,722] Trial 7 finished with value: -0.28042642605995055 and parameters: {'max_depth': 3, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.5, 'gamma': 0, 'min_child_weight': 3, 'lambda': 5, 'alpha': 1, 'n_estimators': 3000}. Best is trial 0 with value: -0.35580943834780965.\n",
      "[I 2025-01-03 06:38:25,766] Trial 2 finished with value: -0.2653074914753838 and parameters: {'max_depth': 3, 'eta': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 3, 'lambda': 5, 'alpha': 0, 'n_estimators': 3000}. Best is trial 0 with value: -0.35580943834780965.\n",
      "[I 2025-01-03 06:43:13,046] Trial 13 finished with value: -0.37361458361806965 and parameters: {'max_depth': 11, 'eta': 0.05, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 10, 'lambda': 10, 'alpha': 10, 'n_estimators': 500}. Best is trial 13 with value: -0.37361458361806965.\n",
      "[I 2025-01-03 06:44:30,363] Trial 22 finished with value: -0.25741991965975836 and parameters: {'max_depth': 7, 'eta': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.5, 'gamma': 0.1, 'min_child_weight': 3, 'lambda': 10, 'alpha': 0, 'n_estimators': 50}. Best is trial 13 with value: -0.37361458361806965.\n",
      "[I 2025-01-03 06:46:23,769] Trial 20 finished with value: -0.2645996667314311 and parameters: {'max_depth': 5, 'eta': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.6, 'gamma': 0.2, 'min_child_weight': 10, 'lambda': 10, 'alpha': 0, 'n_estimators': 500}. Best is trial 13 with value: -0.37361458361806965.\n",
      "[I 2025-01-03 06:47:12,004] Trial 19 finished with value: -0.3749397648045687 and parameters: {'max_depth': 9, 'eta': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 0, 'min_child_weight': 3, 'lambda': 5, 'alpha': 5, 'n_estimators': 500}. Best is trial 19 with value: -0.3749397648045687.\n",
      "[I 2025-01-03 06:47:45,389] Trial 24 finished with value: -0.23102802643552614 and parameters: {'max_depth': 3, 'eta': 0.3, 'subsample': 0.5, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 5, 'lambda': 1, 'alpha': 0, 'n_estimators': 100}. Best is trial 19 with value: -0.3749397648045687.\n",
      "[I 2025-01-03 06:57:39,851] Trial 3 finished with value: -0.38180877269033875 and parameters: {'max_depth': 19, 'eta': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0, 'min_child_weight': 10, 'lambda': 0, 'alpha': 5, 'n_estimators': 3000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:04:41,528] Trial 25 finished with value: -0.35260602645018174 and parameters: {'max_depth': 9, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.5, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 10, 'alpha': 10, 'n_estimators': 500}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:06:35,878] Trial 28 finished with value: -0.3096752508857715 and parameters: {'max_depth': 9, 'eta': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.5, 'gamma': 0.2, 'min_child_weight': 1, 'lambda': 10, 'alpha': 0, 'n_estimators': 50}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:14:16,770] Trial 27 finished with value: -0.33285496983197604 and parameters: {'max_depth': 5, 'eta': 0.3, 'subsample': 0.8, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 5, 'lambda': 1, 'alpha': 5, 'n_estimators': 1000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:17:00,200] Trial 6 finished with value: -0.32849812373018505 and parameters: {'max_depth': 19, 'eta': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.8, 'gamma': 0, 'min_child_weight': 10, 'lambda': 0, 'alpha': 10, 'n_estimators': 3000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:19:07,497] Trial 11 finished with value: -0.35428523173564147 and parameters: {'max_depth': 7, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.8, 'gamma': 0, 'min_child_weight': 5, 'lambda': 0, 'alpha': 0, 'n_estimators': 3000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:24:45,314] Trial 29 finished with value: -0.3673016683000821 and parameters: {'max_depth': 9, 'eta': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.5, 'gamma': 1.0, 'min_child_weight': 1, 'lambda': 5, 'alpha': 1, 'n_estimators': 1000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:26:02,304] Trial 33 finished with value: -0.26759778008691476 and parameters: {'max_depth': 7, 'eta': 0.01, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0.1, 'min_child_weight': 10, 'lambda': 1, 'alpha': 10, 'n_estimators': 50}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:26:46,519] Trial 32 finished with value: -0.32377935618594633 and parameters: {'max_depth': 5, 'eta': 0.3, 'subsample': 1.0, 'colsample_bytree': 0.6, 'gamma': 1.0, 'min_child_weight': 1, 'lambda': 1, 'alpha': 0, 'n_estimators': 500}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:29:25,346] Trial 30 finished with value: -0.27518213912194606 and parameters: {'max_depth': 9, 'eta': 0.5, 'subsample': 0.5, 'colsample_bytree': 0.5, 'gamma': 0.1, 'min_child_weight': 5, 'lambda': 10, 'alpha': 10, 'n_estimators': 500}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:30:54,447] Trial 23 finished with value: -0.3121910531010753 and parameters: {'max_depth': 5, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 1.0, 'gamma': 0.1, 'min_child_weight': 5, 'lambda': 1, 'alpha': 5, 'n_estimators': 3000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:32:07,591] Trial 17 finished with value: -0.35641937652799627 and parameters: {'max_depth': 7, 'eta': 0.05, 'subsample': 0.5, 'colsample_bytree': 0.7, 'gamma': 0.1, 'min_child_weight': 10, 'lambda': 1, 'alpha': 5, 'n_estimators': 3000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:33:40,855] Trial 38 finished with value: -0.29156152946264874 and parameters: {'max_depth': 9, 'eta': 0.05, 'subsample': 0.6, 'colsample_bytree': 0.5, 'gamma': 0.5, 'min_child_weight': 3, 'lambda': 5, 'alpha': 5, 'n_estimators': 50}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:35:34,326] Trial 4 finished with value: -0.3642308675898055 and parameters: {'max_depth': 17, 'eta': 0.1, 'subsample': 0.5, 'colsample_bytree': 0.7, 'gamma': 1.0, 'min_child_weight': 5, 'lambda': 10, 'alpha': 10, 'n_estimators': 3000}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:39:41,520] Trial 40 finished with value: -0.3348306449489193 and parameters: {'max_depth': 13, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 7, 'lambda': 0, 'alpha': 0, 'n_estimators': 50}. Best is trial 3 with value: -0.38180877269033875.\n",
      "[I 2025-01-03 07:40:22,255] Trial 21 finished with value: -0.3906597291928824 and parameters: {'max_depth': 11, 'eta': 0.05, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.5, 'min_child_weight': 5, 'lambda': 0, 'alpha': 5, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 07:41:22,587] Trial 26 finished with value: -0.37621840511779 and parameters: {'max_depth': 19, 'eta': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.2, 'min_child_weight': 10, 'lambda': 0, 'alpha': 10, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 07:46:25,061] Trial 10 finished with value: -0.318499917497636 and parameters: {'max_depth': 17, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 1, 'lambda': 0, 'alpha': 0, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 07:52:55,469] Trial 36 finished with value: -0.38109133285282415 and parameters: {'max_depth': 13, 'eta': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.2, 'min_child_weight': 10, 'lambda': 0, 'alpha': 1, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 07:53:53,462] Trial 45 finished with value: -0.25131514623798396 and parameters: {'max_depth': 5, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'gamma': 0.5, 'min_child_weight': 5, 'lambda': 10, 'alpha': 1, 'n_estimators': 50}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 07:53:56,720] Trial 43 finished with value: -0.3640233047216418 and parameters: {'max_depth': 9, 'eta': 0.3, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.1, 'min_child_weight': 10, 'lambda': 1, 'alpha': 0, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 07:54:05,439] Trial 35 finished with value: -0.3632418805251391 and parameters: {'max_depth': 7, 'eta': 0.3, 'subsample': 1.0, 'colsample_bytree': 1.0, 'gamma': 1.0, 'min_child_weight': 3, 'lambda': 0, 'alpha': 5, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 07:56:57,047] Trial 46 finished with value: -0.292059225159563 and parameters: {'max_depth': 13, 'eta': 0.3, 'subsample': 0.5, 'colsample_bytree': 0.7, 'gamma': 0.1, 'min_child_weight': 7, 'lambda': 5, 'alpha': 10, 'n_estimators': 50}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:03:02,504] Trial 37 finished with value: -0.3728040209286444 and parameters: {'max_depth': 13, 'eta': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.5, 'min_child_weight': 5, 'lambda': 10, 'alpha': 5, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:12:18,386] Trial 34 finished with value: -0.38190435269792344 and parameters: {'max_depth': 17, 'eta': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0.5, 'min_child_weight': 3, 'lambda': 10, 'alpha': 5, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:17:05,117] Trial 39 finished with value: -0.3133499150857365 and parameters: {'max_depth': 5, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 1, 'lambda': 0, 'alpha': 5, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:18:01,818] Trial 52 finished with value: -0.211377400208897 and parameters: {'max_depth': 5, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.9, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 10, 'alpha': 5, 'n_estimators': 50}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:20:10,610] Trial 51 finished with value: -0.35430379004536533 and parameters: {'max_depth': 15, 'eta': 0.3, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.1, 'min_child_weight': 10, 'lambda': 10, 'alpha': 5, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:21:49,873] Trial 42 finished with value: -0.3705597542903083 and parameters: {'max_depth': 15, 'eta': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.9, 'gamma': 0.5, 'min_child_weight': 3, 'lambda': 0, 'alpha': 1, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:23:11,256] Trial 47 finished with value: -0.3549971866855546 and parameters: {'max_depth': 9, 'eta': 0.01, 'subsample': 0.6, 'colsample_bytree': 0.9, 'gamma': 1.0, 'min_child_weight': 10, 'lambda': 1, 'alpha': 0, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:25:14,365] Trial 55 finished with value: -0.3117222516746707 and parameters: {'max_depth': 9, 'eta': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 1, 'lambda': 10, 'alpha': 10, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:31:28,598] Trial 50 finished with value: -0.3660818428210727 and parameters: {'max_depth': 9, 'eta': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'min_child_weight': 5, 'lambda': 10, 'alpha': 5, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:32:49,899] Trial 58 finished with value: -0.2084108380377657 and parameters: {'max_depth': 3, 'eta': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0, 'min_child_weight': 1, 'lambda': 0, 'alpha': 1, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:33:28,009] Trial 53 finished with value: -0.3422805638381093 and parameters: {'max_depth': 19, 'eta': 0.5, 'subsample': 0.8, 'colsample_bytree': 0.9, 'gamma': 0.5, 'min_child_weight': 3, 'lambda': 5, 'alpha': 5, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:36:07,875] Trial 54 finished with value: -0.3306956787166794 and parameters: {'max_depth': 5, 'eta': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.7, 'gamma': 0.1, 'min_child_weight': 3, 'lambda': 10, 'alpha': 1, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:42:38,928] Trial 56 finished with value: -0.3748535323840293 and parameters: {'max_depth': 17, 'eta': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0, 'gamma': 1.0, 'min_child_weight': 3, 'lambda': 5, 'alpha': 10, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:43:54,762] Trial 41 finished with value: -0.33466775780540614 and parameters: {'max_depth': 15, 'eta': 0.01, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.1, 'min_child_weight': 1, 'lambda': 5, 'alpha': 0, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:45:47,158] Trial 31 finished with value: -0.37398158046867425 and parameters: {'max_depth': 9, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 1, 'alpha': 0, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:47:59,994] Trial 63 finished with value: -0.3375823661281821 and parameters: {'max_depth': 13, 'eta': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.9, 'gamma': 0.1, 'min_child_weight': 7, 'lambda': 10, 'alpha': 5, 'n_estimators': 50}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:52:02,637] Trial 44 finished with value: -0.35464334767092065 and parameters: {'max_depth': 9, 'eta': 0.3, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 5, 'lambda': 10, 'alpha': 10, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:53:43,825] Trial 64 finished with value: -0.3265484976630725 and parameters: {'max_depth': 5, 'eta': 0.3, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 5, 'alpha': 10, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:55:47,766] Trial 59 finished with value: -0.2910886095299494 and parameters: {'max_depth': 15, 'eta': 0.3, 'subsample': 0.5, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 3, 'lambda': 1, 'alpha': 0, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:58:47,965] Trial 66 finished with value: -0.3590480686492533 and parameters: {'max_depth': 13, 'eta': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.5, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 5, 'alpha': 5, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 08:59:08,927] Trial 49 finished with value: -0.37183169593125515 and parameters: {'max_depth': 7, 'eta': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.1, 'min_child_weight': 3, 'lambda': 0, 'alpha': 5, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:00:40,703] Trial 65 finished with value: -0.3445912406281192 and parameters: {'max_depth': 19, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.7, 'gamma': 0, 'min_child_weight': 10, 'lambda': 5, 'alpha': 0, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:05:32,187] Trial 71 finished with value: -0.33079536661260167 and parameters: {'max_depth': 11, 'eta': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 7, 'lambda': 1, 'alpha': 5, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:06:36,376] Trial 70 finished with value: -0.3065417277492294 and parameters: {'max_depth': 5, 'eta': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'min_child_weight': 3, 'lambda': 1, 'alpha': 0, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:10:02,543] Trial 61 finished with value: -0.34670384329641585 and parameters: {'max_depth': 17, 'eta': 0.1, 'subsample': 0.5, 'colsample_bytree': 0.6, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 10, 'alpha': 0, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:16:01,102] Trial 62 finished with value: -0.29354598335389914 and parameters: {'max_depth': 3, 'eta': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 10, 'lambda': 1, 'alpha': 1, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:16:43,730] Trial 68 finished with value: -0.34289203031904725 and parameters: {'max_depth': 15, 'eta': 0.3, 'subsample': 0.6, 'colsample_bytree': 0.9, 'gamma': 0, 'min_child_weight': 10, 'lambda': 10, 'alpha': 10, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:17:12,113] Trial 57 finished with value: -0.3803249418095869 and parameters: {'max_depth': 17, 'eta': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.7, 'gamma': 0.1, 'min_child_weight': 5, 'lambda': 10, 'alpha': 5, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:17:25,003] Trial 60 finished with value: -0.34233454381591233 and parameters: {'max_depth': 5, 'eta': 0.3, 'subsample': 0.9, 'colsample_bytree': 0.5, 'gamma': 0, 'min_child_weight': 10, 'lambda': 5, 'alpha': 0, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:19:05,062] Trial 78 finished with value: -0.28959829783683155 and parameters: {'max_depth': 5, 'eta': 0.3, 'subsample': 0.8, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 1, 'lambda': 1, 'alpha': 5, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:31:20,048] Trial 48 finished with value: -0.3575824931274198 and parameters: {'max_depth': 13, 'eta': 0.1, 'subsample': 0.5, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 1, 'lambda': 10, 'alpha': 10, 'n_estimators': 3000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:33:01,321] Trial 80 finished with value: -0.2897548480757285 and parameters: {'max_depth': 5, 'eta': 0.3, 'subsample': 0.7, 'colsample_bytree': 0.9, 'gamma': 0.5, 'min_child_weight': 3, 'lambda': 1, 'alpha': 10, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:35:07,151] Trial 73 finished with value: -0.35443923900998103 and parameters: {'max_depth': 9, 'eta': 0.01, 'subsample': 0.5, 'colsample_bytree': 0.9, 'gamma': 0, 'min_child_weight': 3, 'lambda': 1, 'alpha': 0, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:36:11,780] Trial 75 finished with value: -0.3265176632774448 and parameters: {'max_depth': 7, 'eta': 0.01, 'subsample': 0.5, 'colsample_bytree': 0.5, 'gamma': 1.0, 'min_child_weight': 5, 'lambda': 5, 'alpha': 1, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:37:25,044] Trial 77 finished with value: -0.34869852768452786 and parameters: {'max_depth': 11, 'eta': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.8, 'gamma': 0.2, 'min_child_weight': 3, 'lambda': 10, 'alpha': 5, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:39:16,007] Trial 69 finished with value: -0.3760079525616073 and parameters: {'max_depth': 17, 'eta': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.6, 'gamma': 0.2, 'min_child_weight': 10, 'lambda': 1, 'alpha': 10, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:44:47,437] Trial 81 finished with value: -0.34854138554073 and parameters: {'max_depth': 19, 'eta': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.5, 'min_child_weight': 10, 'lambda': 10, 'alpha': 1, 'n_estimators': 100}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:50:03,182] Trial 79 finished with value: -0.35923852713983223 and parameters: {'max_depth': 11, 'eta': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.9, 'gamma': 0.2, 'min_child_weight': 7, 'lambda': 5, 'alpha': 5, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:53:13,645] Trial 83 finished with value: -0.3569326578170603 and parameters: {'max_depth': 11, 'eta': 0.3, 'subsample': 0.8, 'colsample_bytree': 0.7, 'gamma': 0.2, 'min_child_weight': 5, 'lambda': 10, 'alpha': 5, 'n_estimators': 500}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:54:19,380] Trial 85 finished with value: -0.3231515756437401 and parameters: {'max_depth': 5, 'eta': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 5, 'lambda': 0, 'alpha': 0, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 09:54:30,920] Trial 74 finished with value: -0.3602443033714461 and parameters: {'max_depth': 11, 'eta': 0.01, 'subsample': 0.9, 'colsample_bytree': 0.6, 'gamma': 0.1, 'min_child_weight': 3, 'lambda': 0, 'alpha': 5, 'n_estimators': 1000}. Best is trial 21 with value: -0.3906597291928824.\n",
      "[I 2025-01-03 10:01:26,624] Trial 67 finished with value: -0.39661591621523623 and parameters: {'max_depth': 17, 'eta': 0.05, 'subsample': 1.0, 'colsample_bytree': 0.6, 'gamma': 0.1, 'min_child_weight': 10, 'lambda': 0, 'alpha': 5, 'n_estimators': 3000}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:01:48,680] Trial 88 finished with value: -0.305533247857904 and parameters: {'max_depth': 15, 'eta': 0.01, 'subsample': 0.5, 'colsample_bytree': 0.5, 'gamma': 1.0, 'min_child_weight': 10, 'lambda': 10, 'alpha': 1, 'n_estimators': 100}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:04:04,628] Trial 92 finished with value: -0.3184763065568174 and parameters: {'max_depth': 7, 'eta': 0.1, 'subsample': 0.6, 'colsample_bytree': 1.0, 'gamma': 0.5, 'min_child_weight': 10, 'lambda': 0, 'alpha': 0, 'n_estimators': 100}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:04:46,880] Trial 82 finished with value: -0.3437107687427293 and parameters: {'max_depth': 13, 'eta': 0.3, 'subsample': 0.7, 'colsample_bytree': 0.5, 'gamma': 0.2, 'min_child_weight': 7, 'lambda': 5, 'alpha': 5, 'n_estimators': 1000}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:08:25,474] Trial 89 finished with value: -0.36243688571345095 and parameters: {'max_depth': 9, 'eta': 0.05, 'subsample': 0.9, 'colsample_bytree': 1.0, 'gamma': 0, 'min_child_weight': 1, 'lambda': 5, 'alpha': 10, 'n_estimators': 500}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:12:46,081] Trial 91 finished with value: -0.36527906132738935 and parameters: {'max_depth': 11, 'eta': 0.3, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 10, 'lambda': 10, 'alpha': 0, 'n_estimators': 500}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:13:44,503] Trial 95 finished with value: -0.34918947084468555 and parameters: {'max_depth': 19, 'eta': 0.3, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 7, 'lambda': 5, 'alpha': 5, 'n_estimators': 50}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:15:08,447] Trial 76 finished with value: -0.3470724551587487 and parameters: {'max_depth': 7, 'eta': 0.1, 'subsample': 0.5, 'colsample_bytree': 0.6, 'gamma': 0.2, 'min_child_weight': 10, 'lambda': 5, 'alpha': 0, 'n_estimators': 3000}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:17:05,551] Trial 93 finished with value: -0.31796253657662144 and parameters: {'max_depth': 9, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.9, 'gamma': 0.5, 'min_child_weight': 7, 'lambda': 5, 'alpha': 10, 'n_estimators': 500}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:18:08,708] Trial 98 finished with value: -0.13167900913818356 and parameters: {'max_depth': 17, 'eta': 0.5, 'subsample': 0.5, 'colsample_bytree': 1.0, 'gamma': 1.0, 'min_child_weight': 5, 'lambda': 1, 'alpha': 1, 'n_estimators': 50}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:19:59,010] Trial 84 finished with value: -0.3574290229131362 and parameters: {'max_depth': 17, 'eta': 0.05, 'subsample': 0.5, 'colsample_bytree': 0.7, 'gamma': 0.1, 'min_child_weight': 3, 'lambda': 5, 'alpha': 5, 'n_estimators': 500}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:23:55,736] Trial 99 finished with value: -0.3165017884399305 and parameters: {'max_depth': 17, 'eta': 0.2, 'subsample': 0.5, 'colsample_bytree': 0.9, 'gamma': 1.0, 'min_child_weight': 1, 'lambda': 0, 'alpha': 5, 'n_estimators': 100}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:24:21,673] Trial 97 finished with value: -0.32266475591450516 and parameters: {'max_depth': 17, 'eta': 0.05, 'subsample': 1.0, 'colsample_bytree': 0.9, 'gamma': 0.5, 'min_child_weight': 5, 'lambda': 5, 'alpha': 5, 'n_estimators': 100}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:24:24,129] Trial 94 finished with value: -0.3802452261899174 and parameters: {'max_depth': 13, 'eta': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 1.0, 'min_child_weight': 7, 'lambda': 5, 'alpha': 10, 'n_estimators': 500}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:28:32,919] Trial 96 finished with value: -0.3529323978358947 and parameters: {'max_depth': 17, 'eta': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.5, 'min_child_weight': 5, 'lambda': 5, 'alpha': 0, 'n_estimators': 500}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:43:34,408] Trial 87 finished with value: -0.3728449596351485 and parameters: {'max_depth': 11, 'eta': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.6, 'gamma': 0.2, 'min_child_weight': 7, 'lambda': 5, 'alpha': 10, 'n_estimators': 3000}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 10:49:26,875] Trial 86 finished with value: -0.3846588499931968 and parameters: {'max_depth': 17, 'eta': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.8, 'gamma': 0.5, 'min_child_weight': 3, 'lambda': 0, 'alpha': 10, 'n_estimators': 3000}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 11:03:03,186] Trial 72 finished with value: -0.37990135109232426 and parameters: {'max_depth': 15, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.5, 'min_child_weight': 3, 'lambda': 10, 'alpha': 0, 'n_estimators': 3000}. Best is trial 67 with value: -0.39661591621523623.\n",
      "[I 2025-01-03 11:05:08,387] Trial 90 finished with value: -0.3863085186960519 and parameters: {'max_depth': 15, 'eta': 0.01, 'subsample': 0.7, 'colsample_bytree': 0.8, 'gamma': 0.5, 'min_child_weight': 5, 'lambda': 1, 'alpha': 10, 'n_estimators': 3000}. Best is trial 67 with value: -0.39661591621523623.\n",
      "Best Parameters: {'max_depth': 17, 'eta': 0.05, 'subsample': 1.0, 'colsample_bytree': 0.6, 'gamma': 0.1, 'min_child_weight': 10, 'lambda': 0, 'alpha': 5, 'n_estimators': 3000}\n",
      "Best Score (Negative Average Precision): -0.39661591621523623\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score #use average precision score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space for Optuna using the same values as before\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 19, step=2)  # Corrected range for step=2\n",
    "    eta = trial.suggest_categorical(\"eta\", [0.01, 0.05, 0.1, 0.2, 0.3, 0.5])\n",
    "    subsample = trial.suggest_categorical(\"subsample\", [0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "    colsample_bytree = trial.suggest_categorical(\"colsample_bytree\", [0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0, 0.1, 0.2, 0.5, 1.0])\n",
    "    min_child_weight = trial.suggest_categorical(\"min_child_weight\", [1, 3, 5, 7, 10])\n",
    "    lambda_val = trial.suggest_categorical(\"lambda\", [0, 1, 5, 10])\n",
    "    alpha = trial.suggest_categorical(\"alpha\", [0, 1, 5, 10])\n",
    "    n_estimators = trial.suggest_categorical(\"n_estimators\", [50, 100, 500, 1000, 3000])\n",
    "\n",
    "\n",
    "    # Stratified K-Fold for Cross-Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Evaluate the model using cross-validation using average precision and DMatrix\n",
    "    aucpr_score = 0\n",
    "    for train_index, test_index in cv.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        # Create DMatrices\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # XGBoost parameters (using trial suggestions)\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'aucpr',\n",
    "            'seed': 42,\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'tree_method': 'hist',  # Ensures GPU acceleration\n",
    "            'nthread': -1,\n",
    "            'max_depth': max_depth,\n",
    "            'eta': eta,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'gamma': gamma,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'lambda': lambda_val,\n",
    "            'alpha': alpha,\n",
    "            'device': 'cuda:1'\n",
    "        }\n",
    "\n",
    "        evals_result = {'test': {'aucpr': []}}\n",
    "        bst = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=n_estimators,\n",
    "            evals=[(dtest, 'test')],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        aucpr_score += np.mean(evals_result['test']['aucpr'])\n",
    "        \n",
    "        del X_train, X_test, y_train, y_test, dtrain, dtest, bst, evals_result\n",
    "\n",
    "    aucpr_score /= cv.n_splits\n",
    "\n",
    "    # Return the negative average precision for minimization (minimize error)\n",
    "    return -aucpr_score\n",
    "\n",
    "\n",
    "study = optuna.create_study(storage=f\"sqlite:///db_{STUDY_NAME}.sqlite3\",\n",
    "                            sampler=optuna.samplers.RandomSampler(seed=42),\n",
    "                            # pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    "                            study_name=STUDY_NAME,\n",
    "                            load_if_exists=True,\n",
    "                            direction='minimize')\n",
    "study.optimize(objective, \n",
    "               n_trials=100,\n",
    "               timeout=None,\n",
    "               n_jobs = 10,\n",
    "               gc_after_trial=True,\n",
    "               show_progress_bar = True)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Score (Negative Average Precision):\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_params = study.best_params\n",
    "best_xgb_clf = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        seed=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        device='cuda',\n",
    "        tree_method='hist',  # Ensures GPU acceleration\n",
    "        use_label_encoder=False,\n",
    "        n_jobs=10,\n",
    "        **best_params\n",
    ")\n",
    "best_xgb_clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, 20],\n",
    "    'eta': [0.01, 0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7, 10],\n",
    "    'lambda': [0, 1, 5, 10],\n",
    "    'alpha': [0, 1, 5, 10],\n",
    "    'n_estimators': [50, 100, 500, 1000, 3000],\n",
    "}\n",
    "\n",
    "# XGBClassifier with GPU-specific settings\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='prauc',\n",
    "    seed=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    device='cuda',\n",
    "    tree_method='hist',  # Ensures GPU acceleration\n",
    "    # predictor='gpu_predictor',  # GPU prediction\n",
    "    n_jobs=10\n",
    ")\n",
    "\n",
    "# RandomizedSearchCV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_grid,\n",
    "    scoring='average_precision',\n",
    "    n_iter=100,  # Number of parameter settings sampled\n",
    "    cv=5,       # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=10\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(features, labels)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_feat_tok_df_vectors = spark.read.format('parquet').load(chunks_path[1])\n",
    "# print(full_feat_tok_df_vectors.count())\n",
    "# full_feat_tok_df_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = joblib.load(\"Incrementally_train_XGB2_ckpt/Final_ckpt_params.joblib\")['params']\n",
    "model = xgb.Booster(params=params)\n",
    "model.load_model(\"Incrementally_train_XGB2_ckpt/Final_ckpt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [19:21:53] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1733179637554/work/src/learner.cc:740: \n",
      "Parameters: { \"num_boost_round\", \"rate_drop\", \"skip_drop\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "Chunk: 200: 100%|| 200/200 [7:26:50<00:00, 134.05s/it, 295246830 (100.00%) samples used,0 (0.00%) samples remaining]          \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "full_preds = incrementally_inference(model, chunks_path, save_dir=\"Incrementally_Inference3_Preds\", test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0.152128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191.0</td>\n",
       "      <td>0.017978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418.0</td>\n",
       "      <td>0.010289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>541.0</td>\n",
       "      <td>0.069637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>558.0</td>\n",
       "      <td>0.064629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476723</th>\n",
       "      <td>295246296.0</td>\n",
       "      <td>0.370344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476724</th>\n",
       "      <td>295246331.0</td>\n",
       "      <td>0.045317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476725</th>\n",
       "      <td>295246454.0</td>\n",
       "      <td>0.609166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476726</th>\n",
       "      <td>295246584.0</td>\n",
       "      <td>0.015393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476727</th>\n",
       "      <td>295246670.0</td>\n",
       "      <td>0.005484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1476728 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     binds\n",
       "0               65.0  0.152128\n",
       "1              191.0  0.017978\n",
       "2              418.0  0.010289\n",
       "3              541.0  0.069637\n",
       "4              558.0  0.064629\n",
       "...              ...       ...\n",
       "1476723  295246296.0  0.370344\n",
       "1476724  295246331.0  0.045317\n",
       "1476725  295246454.0  0.609166\n",
       "1476726  295246584.0  0.015393\n",
       "1476727  295246670.0  0.005484\n",
       "\n",
       "[1476728 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_pred_df = pd.DataFrame(chunk_preds)\n",
    "chunk_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.152128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191</td>\n",
       "      <td>0.017978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418</td>\n",
       "      <td>0.010289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>541</td>\n",
       "      <td>0.069637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>558</td>\n",
       "      <td>0.064629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476723</th>\n",
       "      <td>295246296</td>\n",
       "      <td>0.370344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476724</th>\n",
       "      <td>295246331</td>\n",
       "      <td>0.045317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476725</th>\n",
       "      <td>295246454</td>\n",
       "      <td>0.609166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476726</th>\n",
       "      <td>295246584</td>\n",
       "      <td>0.015393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476727</th>\n",
       "      <td>295246670</td>\n",
       "      <td>0.005484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1476728 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     binds\n",
       "0               65  0.152128\n",
       "1              191  0.017978\n",
       "2              418  0.010289\n",
       "3              541  0.069637\n",
       "4              558  0.064629\n",
       "...            ...       ...\n",
       "1476723  295246296  0.370344\n",
       "1476724  295246331  0.045317\n",
       "1476725  295246454  0.609166\n",
       "1476726  295246584  0.015393\n",
       "1476727  295246670  0.005484\n",
       "\n",
       "[1476728 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_pred_df['id'] = chunk_pred_df['id'].astype(np.int64)\n",
    "chunk_pred_df = chunk_pred_df.sort_values(by=['id'])\n",
    "chunk_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|       id|             vectors|\n",
      "+---------+--------------------+\n",
      "|295246852|(190,[0,1,2,3,4,5...|\n",
      "|295246961|(190,[0,1,2,3,4,5...|\n",
      "|295247142|(190,[0,1,2,3,4,5...|\n",
      "|295247169|(190,[0,1,2,3,4,5...|\n",
      "|295247204|(190,[0,1,2,3,4,5...|\n",
      "|295247213|(190,[0,1,2,3,4,5...|\n",
      "|295247329|(190,[0,1,2,3,4,5...|\n",
      "|295247347|(190,[0,1,2,3,4,5...|\n",
      "|295247378|(190,[0,1,2,3,4,5...|\n",
      "|295247397|(190,[0,1,2,3,4,5...|\n",
      "|295247414|(190,[0,1,2,3,4,5...|\n",
      "|295247424|(190,[0,1,2,3,4,5...|\n",
      "|295247425|(190,[0,1,2,3,4,5...|\n",
      "|295247435|(190,[0,1,2,3,4,5...|\n",
      "|295247608|(190,[0,1,2,3,4,5...|\n",
      "|295247672|(190,[0,1,2,3,4,5...|\n",
      "|295247725|(190,[0,1,2,3,4,5...|\n",
      "|295247799|(190,[0,1,2,3,4,5...|\n",
      "|295247807|(190,[0,1,2,3,4,5...|\n",
      "|295247924|(190,[0,1,2,3,4,5...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_feat_tok_df_vectors = spark.read.format('parquet').load(\"test_feat_tok_df_vectors.parquet\")\n",
    "print(test_feat_tok_df_vectors.count())\n",
    "test_feat_tok_df_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = joblib.load(\"Incrementally_train_XGB2_ckpt/Final_ckpt_params.joblib\")['params']\n",
    "model = xgb.Booster(params=params)\n",
    "model.load_model(\"Incrementally_train_XGB2_ckpt/Final_ckpt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk: 1:   0%|          | 1/200 [02:23<7:55:40, 143.42s/it, 1674896 (0.57%) samples used,293571934 (99.43%) samples remaining]\n"
     ]
    }
   ],
   "source": [
    "full_preds = incrementally_inference(model, [\"test_feat_tok_df_vectors.parquet\"], save_dir=\"Incrementally_Inference_Test_Preds\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295246852.0, 3.3264886071388000133e-12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_preds['id'][0], full_preds['binds'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246852.0</td>\n",
       "      <td>3.326489e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246961.0</td>\n",
       "      <td>6.416114e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295247142.0</td>\n",
       "      <td>1.403722e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295247169.0</td>\n",
       "      <td>9.945667e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295247204.0</td>\n",
       "      <td>1.951050e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921421.0</td>\n",
       "      <td>3.620435e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921567.0</td>\n",
       "      <td>1.376455e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921570.0</td>\n",
       "      <td>4.055630e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921682.0</td>\n",
       "      <td>4.409989e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921694.0</td>\n",
       "      <td>6.833635e-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id         binds\n",
       "0        295246852.0  3.326489e-12\n",
       "1        295246961.0  6.416114e-18\n",
       "2        295247142.0  1.403722e-10\n",
       "3        295247169.0  9.945667e-10\n",
       "4        295247204.0  1.951050e-11\n",
       "...              ...           ...\n",
       "1674891  296921421.0  3.620435e-23\n",
       "1674892  296921567.0  1.376455e-07\n",
       "1674893  296921570.0  4.055630e-04\n",
       "1674894  296921682.0  4.409989e-07\n",
       "1674895  296921694.0  6.833635e-21\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_pred_df = pd.DataFrame(full_preds)\n",
    "chunk_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1532514</th>\n",
       "      <td>295246830</td>\n",
       "      <td>2.180438e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200651</th>\n",
       "      <td>295246831</td>\n",
       "      <td>1.023393e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852914</th>\n",
       "      <td>295246832</td>\n",
       "      <td>1.461992e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401038</th>\n",
       "      <td>295246833</td>\n",
       "      <td>6.264513e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852915</th>\n",
       "      <td>295246834</td>\n",
       "      <td>4.745824e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129744</th>\n",
       "      <td>296921721</td>\n",
       "      <td>7.499293e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150626</th>\n",
       "      <td>296921722</td>\n",
       "      <td>2.210206e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325936</th>\n",
       "      <td>296921723</td>\n",
       "      <td>6.274705e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526651</th>\n",
       "      <td>296921724</td>\n",
       "      <td>5.774928e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300907</th>\n",
       "      <td>296921725</td>\n",
       "      <td>5.015421e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id         binds\n",
       "1532514  295246830  2.180438e-04\n",
       "200651   295246831  1.023393e-05\n",
       "852914   295246832  1.461992e-09\n",
       "401038   295246833  6.264513e-13\n",
       "852915   295246834  4.745824e-17\n",
       "...            ...           ...\n",
       "1129744  296921721  7.499293e-15\n",
       "150626   296921722  2.210206e-18\n",
       "325936   296921723  6.274705e-12\n",
       "526651   296921724  5.774928e-13\n",
       "300907   296921725  5.015421e-13\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_pred_df['id'] = chunk_pred_df['id'].astype(np.int64)\n",
    "chunk_pred_df = chunk_pred_df.sort_values(by=['id'])\n",
    "chunk_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(df, model):\n",
    "    predictions = model.transform(df).select(\"id\", \"prediction\", \"probability\").orderBy('id')\n",
    "    predictions.show(truncate=False)\n",
    "    return predictions\n",
    "\n",
    "def makeSubmission(\n",
    "    test_prob, \n",
    "    file_name,\n",
    "    message\n",
    "):\n",
    "    import subprocess, os    \n",
    "    os.makedirs(\"submission_csv\", exist_ok=True)\n",
    "\n",
    "    sub_df = pd.read_csv('sample_submission.csv.zip')\n",
    "    sub_df.binds = test_prob\n",
    "    sub_df.to_csv(file_name, index=False)\n",
    "    display(pd.read_csv(file_name))\n",
    "    \n",
    "    command = [\n",
    "        \"kaggle\", \"competitions\", \"submit\",\n",
    "        \"-c\", \"leash-BELKA\",\n",
    "        \"-f\", file_name,\n",
    "        \"-m\", message\n",
    "    ]\n",
    "    \n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>2.180438e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>1.023393e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>1.461992e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>6.264513e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>4.745824e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721</td>\n",
       "      <td>7.499293e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722</td>\n",
       "      <td>2.210206e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723</td>\n",
       "      <td>6.274705e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724</td>\n",
       "      <td>5.774928e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725</td>\n",
       "      <td>5.015421e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id         binds\n",
       "0        295246830  2.180438e-04\n",
       "1        295246831  1.023393e-05\n",
       "2        295246832  1.461992e-09\n",
       "3        295246833  6.264513e-13\n",
       "4        295246834  4.745824e-17\n",
       "...            ...           ...\n",
       "1674891  296921721  7.499293e-15\n",
       "1674892  296921722  2.210206e-18\n",
       "1674893  296921723  6.274705e-12\n",
       "1674894  296921724  5.774928e-13\n",
       "1674895  296921725  5.015421e-13\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 55.8M/55.8M [00:16<00:00, 3.62MB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to NeurIPS 2024 - Predict New Medicines with BELKA"
     ]
    }
   ],
   "source": [
    "makeSubmission(\n",
    "    test_prob=chunk_pred_df.binds.values,\n",
    "    file_name= f\"submission_csv/_6_sub_XGBoost_Incrementally-HPS.csv\",\n",
    "    message = f\"XGBoost Incrementally GPU with HPS\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
