{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fdf296bf-a18e-42a1-8db9-b7cfd95c5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25b844da-0211-4803-a956-c2460fa3b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 20:46:19 WARN Utils: Your hostname, kanjur resolves to a loopback address: 127.0.1.1; using 10.119.2.14 instead (on interface eno3)\n",
      "24/12/24 20:46:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 20:46:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/24 20:46:20 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kanjur.ieor.iitb.ac.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[64]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>leash belka3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd705dbc740>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka3\")\n",
    "    .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "    .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "    .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "    .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "    .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "    .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "    .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "    .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f36f78d7-d67d-44f3-bdbe-c6568f7bf6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+---------------------+---------------------+--------------------+------------+\n",
      "|       id|buildingblock1_smiles|buildingblock2_smiles|buildingblock3_smiles|     molecule_smiles|protein_name|\n",
      "+---------+---------------------+---------------------+---------------------+--------------------+------------+\n",
      "|295246830| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       C=Cc1ccc(N)cc1|C#CCCC[C@H](Nc1nc...|        BRD4|\n",
      "|295246831| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       C=Cc1ccc(N)cc1|C#CCCC[C@H](Nc1nc...|         HSA|\n",
      "|295246832| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       C=Cc1ccc(N)cc1|C#CCCC[C@H](Nc1nc...|         sEH|\n",
      "|295246833| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| CC(O)Cn1cnc2c(N)n...|C#CCCC[C@H](Nc1nc...|        BRD4|\n",
      "|295246834| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| CC(O)Cn1cnc2c(N)n...|C#CCCC[C@H](Nc1nc...|         HSA|\n",
      "|295246835| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| CC(O)Cn1cnc2c(N)n...|C#CCCC[C@H](Nc1nc...|         sEH|\n",
      "|295246836| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|     CC1(C)CCCC1(O)CN|C#CCCC[C@H](Nc1nc...|        BRD4|\n",
      "|295246837| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|     CC1(C)CCCC1(O)CN|C#CCCC[C@H](Nc1nc...|         HSA|\n",
      "|295246838| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|     CC1(C)CCCC1(O)CN|C#CCCC[C@H](Nc1nc...|         sEH|\n",
      "|295246839| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|  COC(=O)c1cc(Cl)sc1N|C#CCCC[C@H](Nc1nc...|        BRD4|\n",
      "|295246840| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|  COC(=O)c1cc(Cl)sc1N|C#CCCC[C@H](Nc1nc...|         HSA|\n",
      "|295246841| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|  COC(=O)c1cc(Cl)sc1N|C#CCCC[C@H](Nc1nc...|         sEH|\n",
      "|295246842| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       CSC1CCC(CN)CC1|C#CCCC[C@H](Nc1nc...|        BRD4|\n",
      "|295246843| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       CSC1CCC(CN)CC1|C#CCCC[C@H](Nc1nc...|         HSA|\n",
      "|295246844| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       CSC1CCC(CN)CC1|C#CCCC[C@H](Nc1nc...|         sEH|\n",
      "|295246845| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1ccc(CN)c(N2CCC...|C#CCCC[C@H](Nc1nc...|        BRD4|\n",
      "|295246846| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1ccc(CN)c(N2CCC...|C#CCCC[C@H](Nc1nc...|         HSA|\n",
      "|295246847| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1ccc(CN)c(N2CCC...|C#CCCC[C@H](Nc1nc...|         sEH|\n",
      "|295246848| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1cccc(NC(=O)COc...|C#CCCC[C@H](Nc1nc...|        BRD4|\n",
      "|295246849| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1cccc(NC(=O)COc...|C#CCCC[C@H](Nc1nc...|         HSA|\n",
      "+---------+---------------------+---------------------+---------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load('test.parquet')\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f07cc4d3-bd58-4814-babb-905f8220efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+---------------------+---------------------+--------------------+------------+-----+\n",
      "|       id|buildingblock1_smiles|buildingblock2_smiles|buildingblock3_smiles|     molecule_smiles|protein_name|binds|\n",
      "+---------+---------------------+---------------------+---------------------+--------------------+------------+-----+\n",
      "|295246830| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       C=Cc1ccc(N)cc1|C#CCCC[C@H](Nc1nc...|        BRD4|    2|\n",
      "|295246831| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       C=Cc1ccc(N)cc1|C#CCCC[C@H](Nc1nc...|         HSA|    2|\n",
      "|295246832| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       C=Cc1ccc(N)cc1|C#CCCC[C@H](Nc1nc...|         sEH|    2|\n",
      "|295246833| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| CC(O)Cn1cnc2c(N)n...|C#CCCC[C@H](Nc1nc...|        BRD4|    2|\n",
      "|295246834| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| CC(O)Cn1cnc2c(N)n...|C#CCCC[C@H](Nc1nc...|         HSA|    2|\n",
      "|295246835| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| CC(O)Cn1cnc2c(N)n...|C#CCCC[C@H](Nc1nc...|         sEH|    2|\n",
      "|295246836| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|     CC1(C)CCCC1(O)CN|C#CCCC[C@H](Nc1nc...|        BRD4|    2|\n",
      "|295246837| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|     CC1(C)CCCC1(O)CN|C#CCCC[C@H](Nc1nc...|         HSA|    2|\n",
      "|295246838| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|     CC1(C)CCCC1(O)CN|C#CCCC[C@H](Nc1nc...|         sEH|    2|\n",
      "|295246839| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|  COC(=O)c1cc(Cl)sc1N|C#CCCC[C@H](Nc1nc...|        BRD4|    2|\n",
      "|295246840| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|  COC(=O)c1cc(Cl)sc1N|C#CCCC[C@H](Nc1nc...|         HSA|    2|\n",
      "|295246841| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|  COC(=O)c1cc(Cl)sc1N|C#CCCC[C@H](Nc1nc...|         sEH|    2|\n",
      "|295246842| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       CSC1CCC(CN)CC1|C#CCCC[C@H](Nc1nc...|        BRD4|    2|\n",
      "|295246843| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       CSC1CCC(CN)CC1|C#CCCC[C@H](Nc1nc...|         HSA|    2|\n",
      "|295246844| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1|       CSC1CCC(CN)CC1|C#CCCC[C@H](Nc1nc...|         sEH|    2|\n",
      "|295246845| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1ccc(CN)c(N2CCC...|C#CCCC[C@H](Nc1nc...|        BRD4|    2|\n",
      "|295246846| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1ccc(CN)c(N2CCC...|C#CCCC[C@H](Nc1nc...|         HSA|    2|\n",
      "|295246847| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1ccc(CN)c(N2CCC...|C#CCCC[C@H](Nc1nc...|         sEH|    2|\n",
      "|295246848| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1cccc(NC(=O)COc...|C#CCCC[C@H](Nc1nc...|        BRD4|    2|\n",
      "|295246849| C#CCCC[C@H](NC(=O...|       C=Cc1ccc(N)cc1| Cc1cccc(NC(=O)COc...|C#CCCC[C@H](Nc1nc...|         HSA|    2|\n",
      "+---------+---------------------+---------------------+---------------------+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('binds', lit(2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "99db5b3f-4248-44be-9110-6d05e5c81da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afddc329-d8cc-4c45-bcab-859dea265917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2ea534d8-67de-4df7-be3d-5f877904f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.format('parquet').mode('overwrite').option('header', True).save('test2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520e090-0fc3-43d9-85ef-652e243fb369",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f57f941-1746-4151-9e8c-06938e5e2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import joblib\n",
    "from tqdm.auto import trange, tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "from rich.progress import Progress, BarColumn, TaskProgressColumn, TimeRemainingColumn, TimeElapsedColumn, MofNCompleteColumn\n",
    "import gc\n",
    "\n",
    "protein_map = {'BRD4': 1, 'HSA': 2, 'sEH': 3}\n",
    "vocab = {'C': 6825082866, '#': 81527490, '@': 511451694, 'H': 456489972, '=': 1406606874, 'O': 2554179786, 'N': 2469595230, \n",
    "         'c': 12257477022, '-': 438483636, '.': 216945504, 'l': 491088828, 'B': 123330132, 'r': 121915914, 'n': 1997759694, \n",
    "         'D': 295246830, 'y': 295246830, 'o': 67918650, 's': 156618468, 'S': 90662574, 'F': 492710238, '+': 65206260, \n",
    "         'i': 1414026, '/': 11547096, 'I': 23972994}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091d0237-500d-422c-8909-143a6d425547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(dff, update=None):\n",
    "    letter_counts = Counter(update) if update else Counter()\n",
    "    l = dff.drop(columns=['id', 'protein_name', 'binds']).to_numpy().flatten()\n",
    "    l = np.char.replace(l, r'[\\d()\\[\\]{}]+', '', regex=True)\n",
    "    letter_counts.update(''.join(l))\n",
    "    return dict(letter_counts)\n",
    "\n",
    "def make_counter(l):\n",
    "    l = re.sub(r'[\\d()\\[\\]{}]+', '', ''.join(l))\n",
    "    return dict(Counter(l))\n",
    "\n",
    "def allign_counter_to_vocab(counter, vocab):\n",
    "    return {key: counter.get(key, 0) for key in vocab.keys()}\n",
    "\n",
    "def make_features(df, vocab):\n",
    "    id = df['id'].to_numpy()\n",
    "    smiles = df.drop(columns=['id', 'protein_name', 'binds']).to_numpy()\n",
    "    protein = df['protein_name'].to_numpy()\n",
    "    y = df['binds'].to_numpy()\n",
    "\n",
    "    df_features = {'id':[], 'bb1':[], 'bb2':[], 'bb3':[], 'molecule':[], 'protein':[], 'y':[]}\n",
    "    for i in trange(len(id), desc='making features'):\n",
    "        df_features['id'].append(id[i])\n",
    "\n",
    "        counter = make_counter(smiles[i][0])\n",
    "        df_features['bb1'].append(allign_counter_to_vocab(counter, vocab))\n",
    "\n",
    "        counter = make_counter(smiles[i][1])\n",
    "        df_features['bb2'].append(allign_counter_to_vocab(counter, vocab))\n",
    "\n",
    "        counter = make_counter(smiles[i][2])\n",
    "        df_features['bb3'].append(allign_counter_to_vocab(counter, vocab))\n",
    "\n",
    "        counter = make_counter(smiles[i][3])\n",
    "        df_features['molecule'].append(allign_counter_to_vocab(counter, vocab))\n",
    "\n",
    "        df_features['protein'].append(protein[i])\n",
    "        df_features['y'].append(y[i])\n",
    "\n",
    "    return df_features\n",
    "\n",
    "def check_df_allignment(dff_features, vocab):\n",
    "    flag = True\n",
    "    for i in trange(len(dff_features['bb1'])):\n",
    "        if dff_features['bb1'][i].keys() != vocab.keys():\n",
    "            print(dff_features['bb1'][i].keys())\n",
    "            print(vocab.keys())\n",
    "            flag = False\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "def df_vectors(dff_features, vocab, protein_map):\n",
    "    op = np.empty((100,7))\n",
    "    for i in trange(0,len(dff_features['id']),100, desc='Making vector df'):\n",
    "        df = pd.DataFrame({\n",
    "            'id': dff_features['id'][i:i+100],\n",
    "            'bb1': dff_features['bb1'][i:i+100],\n",
    "            'bb2': dff_features['bb2'][i:i+100],\n",
    "            'bb3': dff_features['bb3'][i:i+100],\n",
    "            'molecule': dff_features['molecule'][i:i+100],\n",
    "            'protein': dff_features['protein'][i:i+100],\n",
    "            'y': dff_features['y'][i:i+100]\n",
    "        })\n",
    "\n",
    "        df.bb1 = df.bb1.apply(lambda x: list(x.values()))\n",
    "        df.bb2 = df.bb2.apply(lambda x: list(x.values()))\n",
    "        df.bb3 = df.bb3.apply(lambda x: list(x.values()))\n",
    "        df.molecule = df.molecule.apply(lambda x: list(x.values()))\n",
    "        df.protein = df.protein.map(protein_map)\n",
    "\n",
    "        op = np.concatenate((op, df.to_numpy()))\n",
    "\n",
    "    return op[100:]\n",
    "\n",
    "\n",
    "def process_row(row, protein_map=protein_map):\n",
    "    return {\n",
    "             'id': row['id'],\n",
    "             'bb1': list(allign_counter_to_vocab(make_counter(row['buildingblock1_smiles']), vocab).values()),\n",
    "             'bb2': list(allign_counter_to_vocab(make_counter(row['buildingblock2_smiles']), vocab).values()),\n",
    "             'bb3': list(allign_counter_to_vocab(make_counter(row['buildingblock3_smiles']), vocab).values()),\n",
    "             'molecule': list(allign_counter_to_vocab(make_counter(row['molecule_smiles']), vocab).values()),\n",
    "             'protein': protein_map[row['protein_name']],\n",
    "             'y': row['binds']\n",
    "        }\n",
    "\n",
    "def split(path, frac):\n",
    "    dask_df = dd.read_parquet(path)\n",
    "    train_fraction = frac\n",
    "    train_df, val_df = dask_df.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "    print(f\"Train size: {train_df.shape[0].compute()}\")\n",
    "    print(f\"Validation size: {val_df.shape[0].compute()}\")\n",
    "    train_df.to_parquet(\"train_split.parquet\", write_index=False)\n",
    "    val_df.to_parquet(\"val_split.parquet\", write_index=False)\n",
    "\n",
    "def split2(path, frac=0.5):\n",
    "    dask_df = dd.read_parquet(path)\n",
    "    train_fraction = frac\n",
    "\n",
    "    f1, f2 = dask_df.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "    f3, f4 = f1.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "    f5, f6 = f2.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "\n",
    "    f7, f8 = f3.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "    f9, f10 = f4.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "    f11, f12 = f5.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "    f13, f14 = f6.random_split([train_fraction, 1 - train_fraction], random_state=42)\n",
    "\n",
    "    print(f\"Split-1: {f7.shape[0].compute()}\")\n",
    "    f7.to_parquet(\"Train_Full_Split-1.parquet\", write_index=False)\n",
    "    print(f\"Split-2: {f8.shape[0].compute()}\")\n",
    "    f8.to_parquet(\"Train_Full_Split-2.parquet\", write_index=False)\n",
    "    print(f\"Split-3: {f9.shape[0].compute()}\")\n",
    "    f9.to_parquet(\"Train_Full_Split-3.parquet\", write_index=False)\n",
    "    print(f\"Split-4: {f10.shape[0].compute()}\")\n",
    "    f10.to_parquet(\"Train_Full_Split-4.parquet\", write_index=False)\n",
    "    print(f\"Split-5: {f11.shape[0].compute()}\")\n",
    "    f11.to_parquet(\"Train_Full_Split-5.parquet\", write_index=False)\n",
    "    print(f\"Split-6: {f12.shape[0].compute()}\")\n",
    "    f12.to_parquet(\"Train_Full_Split-6.parquet\", write_index=False)\n",
    "    print(f\"Split-7: {f13.shape[0].compute()}\")\n",
    "    f13.to_parquet(\"Train_Full_Split-7.parquet\", write_index=False)\n",
    "    print(f\"Split-8: {f14.shape[0].compute()}\")\n",
    "    f14.to_parquet(\"Train_Full_Split-8.parquet\", write_index=False)\n",
    "\n",
    "\n",
    "\n",
    "class ParquetDataset(IterableDataset):\n",
    "    def __init__(self, dask_df, vocab=vocab, protein_map=protein_map, transform=None):\n",
    "        self.dask_df = dask_df\n",
    "        self.partitions = self.dask_df.to_delayed()\n",
    "        self.vocab = vocab\n",
    "        self.protein_map = protein_map\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        for partition in self.partitions:\n",
    "            chunk = partition.compute()\n",
    "            for _, row in chunk.iterrows():\n",
    "                yield self.process_row(row)\n",
    "\n",
    "    def process_row(self, row):\n",
    "        data = {\n",
    "            'id': row['id'],\n",
    "            'bb1': list(allign_counter_to_vocab(make_counter(row['buildingblock1_smiles']), self.vocab).values()),\n",
    "            'bb2': list(allign_counter_to_vocab(make_counter(row['buildingblock2_smiles']), self.vocab).values()),\n",
    "            'bb3': list(allign_counter_to_vocab(make_counter(row['buildingblock3_smiles']), self.vocab).values()),\n",
    "            'molecule': list(allign_counter_to_vocab(make_counter(row['molecule_smiles']), self.vocab).values()),\n",
    "            'protein': self.protein_map[row['protein_name']],\n",
    "            'y': row['binds']\n",
    "        }\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e97a0b-b344-4c38-a27f-72ae9f86a5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Processing...</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008000; text-decoration-color: #008000\">1674896/1674896</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:03:39</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mProcessing...\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[32m1674896/1674896\u001b[0m \u001b[33m0:03:39\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dask_df = dd.read_parquet(\"/home/23m1521/ashish/kaggle/test2.parquet\")\n",
    "\n",
    "df_len = dask_df.shape[0].compute()\n",
    "print(f\"Number of rows: {df_len}\")\n",
    "\n",
    "df_dataset = ParquetDataset(dask_df)\n",
    "\n",
    "# Output directory for chunk files\n",
    "output_dir = 'chunks_output_test'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "chunk_size = 1674896  # Size of each chunk\n",
    "chunk_data = []\n",
    "\n",
    "with Progress(\n",
    "    \"[cyan]{task.description}\",\n",
    "    BarColumn(),\n",
    "    TaskProgressColumn(),\n",
    "    MofNCompleteColumn(),\n",
    "    TimeElapsedColumn(),\n",
    "    TimeRemainingColumn(),\n",
    ") as progress:\n",
    "    task = progress.add_task(\"Processing...\", total=df_len)\n",
    "    \n",
    "    for i, data in enumerate(df_dataset):\n",
    "        progress.update(task, advance=1)\n",
    "        chunk_data.append(data)\n",
    "        \n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            # Save the chunk to a separate parquet file\n",
    "            chunk_file = os.path.join(output_dir, f\"chunk_{(i + 1) // chunk_size}.parquet\")\n",
    "            df = pd.DataFrame(chunk_data)\n",
    "            df.to_parquet(chunk_file, engine='pyarrow', compression='snappy', index=False)\n",
    "            print(f\"Saved {chunk_file}\")\n",
    "            \n",
    "            # Free RAM\n",
    "            del chunk_data, df\n",
    "            chunk_data = []\n",
    "            gc.collect()\n",
    "\n",
    "# Save remaining data\n",
    "if chunk_data:\n",
    "    chunk_file = os.path.join(output_dir, f\"chunk_{(df_len // chunk_size) + 1}.parquet\")\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "    df.to_parquet(chunk_file, engine='pyarrow', compression='snappy', index=False)\n",
    "    print(f\"Saved {chunk_file}\")\n",
    "    \n",
    "    # Free RAM\n",
    "    del chunk_data, df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc6339-d87d-4087-b2c0-7b1acc3dbd57",
   "metadata": {},
   "source": [
    "//////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07090d35-c8ee-4d19-b9bf-c26919922220",
   "metadata": {},
   "source": [
    "### Feature Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8564981-8de3-4698-bbf5-daede576afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 295246830\n",
    "one_len = 1589906\n",
    "zero_len = 293656924\n",
    "protein_map = {'BRD4': 1, 'HSA': 2, 'sEH': 3}\n",
    "vocab = {'C': 6825082866, '#': 81527490, '@': 511451694, 'H': 456489972, '=': 1406606874, 'O': 2554179786, \n",
    "         'N': 2469595230, 'c': 12257477022, '-': 438483636, '.': 216945504, 'l': 491088828, 'B': 123330132, \n",
    "         'r': 121915914, 'n': 1997759694, 'D': 295246830, 'y': 295246830, 'o': 67918650, 's': 156618468, \n",
    "         'S': 90662574, 'F': 492710238, '+': 65206260, 'i': 1414026, '/': 11547096, 'I': 23972994}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad9fb7d-9597-4f92-a91f-b09d3709fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType, IntegerType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96e5212-be31-4242-a6a3-3ad432e0c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 21:13:06 WARN Utils: Your hostname, kanjur resolves to a loopback address: 127.0.1.1; using 10.119.2.14 instead (on interface eno3)\n",
      "24/12/24 21:13:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 21:13:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/24 21:13:07 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kanjur.ieor.iitb.ac.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[64]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>leash belka3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbb114ebef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 128 Gb and 32 Cores\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"16g\")\n",
    "#     .config(\"spark.executor.memory\", \"16g\")\n",
    "#     .config(\"spark.executor.instances\", \"4\")\n",
    "#     .config(\"spark.executor.cores\", \"4\")\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "#     .master(\"local[*]\")\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# for 256 Gb and 64 Cores\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka3\")\n",
    "    .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "    .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "    .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "    .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "    .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "    .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "    .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "    .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf5b90f-9865-4c8b-83c6-bdea825b72bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1674896\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "|       id|                 bb1|                 bb2|                 bb3|            molecule|protein|  y|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "|295246830|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[2, 0, 0, 0, 1, 0...|[11, 1, 1, 1, 3, ...|      1|  2|\n",
      "|295246831|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[2, 0, 0, 0, 1, 0...|[11, 1, 1, 1, 3, ...|      2|  2|\n",
      "|295246832|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[2, 0, 0, 0, 1, 0...|[11, 1, 1, 1, 3, ...|      3|  2|\n",
      "|295246833|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[3, 0, 0, 0, 0, 1...|[12, 1, 1, 1, 2, ...|      1|  2|\n",
      "|295246834|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[3, 0, 0, 0, 0, 1...|[12, 1, 1, 1, 2, ...|      2|  2|\n",
      "|295246835|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[3, 0, 0, 0, 0, 1...|[12, 1, 1, 1, 2, ...|      3|  2|\n",
      "|295246836|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 1...|[17, 1, 1, 1, 2, ...|      1|  2|\n",
      "|295246837|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 1...|[17, 1, 1, 1, 2, ...|      2|  2|\n",
      "|295246838|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 1...|[17, 1, 1, 1, 2, ...|      3|  2|\n",
      "|295246839|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[3, 0, 0, 0, 1, 2...|[12, 1, 1, 1, 3, ...|      1|  2|\n",
      "|295246840|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[3, 0, 0, 0, 1, 2...|[12, 1, 1, 1, 3, ...|      2|  2|\n",
      "|295246841|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[3, 0, 0, 0, 1, 2...|[12, 1, 1, 1, 3, ...|      3|  2|\n",
      "|295246842|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 0...|[17, 1, 1, 1, 2, ...|      1|  2|\n",
      "|295246843|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 0...|[17, 1, 1, 1, 2, ...|      2|  2|\n",
      "|295246844|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 0...|[17, 1, 1, 1, 2, ...|      3|  2|\n",
      "|295246845|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 0...|[15, 1, 1, 1, 2, ...|      1|  2|\n",
      "|295246846|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 0...|[15, 1, 1, 1, 2, ...|      2|  2|\n",
      "|295246847|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[8, 0, 0, 0, 0, 0...|[15, 1, 1, 1, 2, ...|      3|  2|\n",
      "|295246848|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[6, 0, 0, 0, 1, 2...|[14, 1, 1, 1, 3, ...|      1|  2|\n",
      "|295246849|[10, 1, 1, 1, 2, ...|[2, 0, 0, 0, 1, 0...|[6, 0, 0, 0, 1, 2...|[14, 1, 1, 1, 3, ...|      2|  2|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load('chunks_output_test')\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e01030f-2ff6-4c13-a864-3af73a5b0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i in range(24):\n",
    "    cols.append(col('bb1').getItem(i).alias(f'a{i+1}'))\n",
    "    cols.append(col('bb2').getItem(i).alias(f'b{i+1}'))\n",
    "    cols.append(col('bb3').getItem(i).alias(f'c{i+1}'))\n",
    "    cols.append(col('molecule').getItem(i).alias(f'd{i+1}'))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('protein', IntegerType(), True),\n",
    "    *[StructField(f'a{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    *[StructField(f'b{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    *[StructField(f'c{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    *[StructField(f'd{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    StructField('y', IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = df.select('id', 'protein', *cols, 'y')\n",
    "df = spark.createDataFrame(df.rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073708eb-6bc4-484b-b8ce-7edaf151dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 21:13:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id=295246830, protein=1, a1=10, a2=2, a3=2, a4=11, a5=1, a6=0, a7=0, a8=1, a9=1, a10=0, a11=0, a12=1, a13=1, a14=0, a15=0, a16=1, a17=2, a18=1, a19=1, a20=3, a21=4, a22=0, a23=0, a24=1, b1=1, b2=1, b3=1, b4=4, b5=12, b6=6, b7=6, b8=15, b9=1, b10=0, b11=0, b12=0, b13=0, b14=0, b15=0, b16=0, b17=0, b18=0, b19=0, b20=0, b21=0, b22=0, b23=0, b24=0, c1=0, c2=0, c3=0, c4=0, c5=0, c6=0, c7=0, c8=3, c9=0, c10=0, c11=0, c12=1, c13=0, c14=0, c15=0, c16=1, c17=0, c18=0, c19=0, c20=0, c21=0, c22=0, c23=0, c24=0, d1=0, d2=0, d3=0, d4=0, d5=0, d6=0, d7=0, d8=0, d9=0, d10=0, d11=0, d12=0, d13=0, d14=0, d15=0, d16=0, d17=0, d18=0, d19=0, d20=0, d21=0, d22=0, d23=0, d24=0, y=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e74d32a-23b0-4e54-982f-25f55ae4b523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af0b83e-ba09-4a91-9fb8-8e4dd3aa67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f472780-f38a-408e-a697-ff9db9c0e04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b67c09-d7d5-4a21-ab4f-dc5ccd5d7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.format('parquet').mode('overwrite').option('header', True).save('test_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdaaf5-5cda-4278-a9d8-cbe447988855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e329b2-be63-4878-8626-0fb4ce8266b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863811a-5671-406d-8ff4-ead19ab76961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5fbb7-5e15-4b69-b161-fe2bb33f5f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b3243-cf79-4a7f-af3d-3b714c44d74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
