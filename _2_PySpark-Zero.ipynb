{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca027266-e09d-4d56-8456-36ff0fb3842f",
   "metadata": {},
   "source": [
    "## Zero Class Features DataFrame Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3cc2ca-9873-4637-8181-c32072ce57e3",
   "metadata": {},
   "source": [
    "### Data Seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9351a4b-26cc-4fa9-bae9-43c56488fbf3",
   "metadata": {},
   "source": [
    "Given your PC's resources (128 GB of RAM and 32 cores), you can optimize the Spark session configuration for better performance when processing large data, particularly a Parquet file and repartitioning it into 184 partitions.\n",
    "\n",
    "### Suggested Configuration:\n",
    "\n",
    "1. **`spark.driver.memory`**:\n",
    "   - You have 128 GB of RAM, so it's safe to allocate a substantial portion to the driver. However, 24 GB is already a good starting point, as the driver does not need as much memory as executors. You can keep this value at `24g` or adjust based on job requirements.\n",
    "\n",
    "2. **`spark.executor.memory`**:\n",
    "   - Each executor should get a reasonable amount of memory without causing memory overuse. With 128 GB of RAM, allocating 4 GB to each executor is reasonable, but since you have 32 cores available, you can increase it to 8-16 GB per executor to fully utilize your machine's capacity.\n",
    "\n",
    "3. **`spark.executor.instances`**:\n",
    "   - You can set this to 8, as you're working with 32 cores. With 8 executors, each executor will get access to 4 cores (`32 cores / 8 executors = 4 cores per executor`), which ensures that each executor is properly utilized.\n",
    "\n",
    "4. **`spark.executor.cores`**:\n",
    "   - With 32 cores available and a total of 8 executors, allocating 4 cores per executor is a good choice, as mentioned above. This ensures that you are using all cores efficiently.\n",
    "\n",
    "5. **`spark.driver.maxResultSize`**:\n",
    "   - With a 128 GB machine, 4 GB for max result size should suffice, as the driver is mainly responsible for coordinating the job, not processing large data.\n",
    "\n",
    "6. **`master`**:\n",
    "   - Using `local[32]` will allow Spark to utilize all the cores on your machine for parallel computation. The `[*]` setting would use all cores, but specifying `local[32]` can give you more control over how many cores to allocate. \n",
    "\n",
    "7. **Repartitioning**:\n",
    "   - When repartitioning to 184 partitions, ensure that you have enough executors to handle the task. With 8 executors and 184 partitions, each executor will handle around 23 partitions (`184 partitions / 8 executors â‰ˆ 23 partitions per executor`). This will help in balancing the load effectively.\n",
    "\n",
    "### Updated Configuration:\n",
    "\n",
    "```python\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka3\")\n",
    "    .config(\"spark.driver.memory\", \"24g\")  # Driver memory (sufficient for your system)\n",
    "    .config(\"spark.executor.memory\", \"16g\")  # Increased executor memory (optimal for large file)\n",
    "    .config(\"spark.executor.instances\", \"8\")  # 8 executors to utilize 32 cores\n",
    "    .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor (balanced with 8 executors)\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Max result size for the driver\n",
    "    .master(\"local[32]\")  # Utilize all 32 cores\n",
    "    .getOrCreate()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330bf9f2-a23a-411b-9dd3-3843f2f8d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3ed01a-98a7-4647-8b2c-67f1c5427d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 00:51:51 WARN Utils: Your hostname, kanjur resolves to a loopback address: 127.0.1.1; using 10.119.2.14 instead (on interface eno3)\n",
      "24/12/24 00:51:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 00:51:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kanjur.ieor.iitb.ac.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[64]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>leash belka3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7e03c4830>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 256 Gb and 64 Cores\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka3\")\n",
    "    .config(\"spark.driver.memory\", \"24g\")  # Driver memory\n",
    "    .config(\"spark.executor.memory\", \"32g\")  # Executor memory (increased for large dataset)\n",
    "    .config(\"spark.executor.instances\", \"16\")  # Number of executors (16 executors for 64 cores)\n",
    "    .config(\"spark.executor.cores\", \"4\")  # Executor cores (4 cores per executor)\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Max result size for driver\n",
    "    # .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "    .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Spark memory fraction (80% of executor memory)\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.4\")  # Shuffle memory fraction (40% of executor memory)\n",
    "    .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75fe34eb-98b8-40af-8dbc-6e7d8c6f315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9348ce-b8de-41ff-8357-d78866d17d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295246830\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "|       id|                 bb1|                 bb2|                 bb3|            molecule|protein|  y|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "|107000000|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 1, 1, 1...|[5, 0, 2, 2, 2, 3...|      3|  0|\n",
      "|107000001|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      1|  0|\n",
      "|107000002|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      2|  0|\n",
      "|107000003|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      3|  0|\n",
      "|107000004|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      1|  0|\n",
      "|107000005|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      2|  0|\n",
      "|107000006|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      3|  0|\n",
      "|107000007|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 1, 1, 1...|[5, 0, 2, 2, 2, 3...|      1|  0|\n",
      "|107000008|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 1, 1, 1...|[5, 0, 2, 2, 2, 3...|      2|  0|\n",
      "|107000009|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 1, 1, 1...|[5, 0, 2, 2, 2, 3...|      3|  0|\n",
      "|107000010|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      1|  0|\n",
      "|107000011|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      2|  0|\n",
      "|107000012|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      3|  0|\n",
      "|107000013|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      1|  0|\n",
      "|107000014|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      2|  0|\n",
      "|107000015|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[5, 0, 2, 1, 1, 2...|      3|  0|\n",
      "|107000016|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[1, 0, 0, 0, 0, 1...|[4, 0, 2, 1, 1, 3...|      1|  0|\n",
      "|107000017|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[1, 0, 0, 0, 0, 1...|[4, 0, 2, 1, 1, 3...|      2|  0|\n",
      "|107000018|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[1, 0, 0, 0, 0, 1...|[4, 0, 2, 1, 1, 3...|      3|  0|\n",
      "|107000019|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 1...|[5, 0, 2, 1, 1, 3...|      1|  0|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load('chunks_output')\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93af081-a055-4e5a-ad08-38dd346a80e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0 = df.where('y == 0')\n",
    "df0 = df0.repartition(184)\n",
    "\n",
    "# print(df0.rdd.getNumPartitions())\n",
    "# print(df0.count())\n",
    "# df0.select('y').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ace13-4009-424f-b959-522d535b2946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014c7a0-77dd-4692-bb98-8a354dbd28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df0.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67de35da-2423-4459-a4c8-acf1a652c9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df0.write.format('parquet').mode('overwrite').option('header', True).save('zero.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302200e5-7014-4dbe-970d-35c11b82ad9d",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21d03b-4025-4948-a672-d02891ed034c",
   "metadata": {},
   "source": [
    "### Feature Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776c1cb0-dcbe-4a95-a120-6f173cf9a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 295246830\n",
    "one_len = 1589906\n",
    "zero_len = 293656924\n",
    "protein_map = {'BRD4': 1, 'HSA': 2, 'sEH': 3}\n",
    "vocab = {'C': 6825082866, '#': 81527490, '@': 511451694, 'H': 456489972, '=': 1406606874, 'O': 2554179786, \n",
    "         'N': 2469595230, 'c': 12257477022, '-': 438483636, '.': 216945504, 'l': 491088828, 'B': 123330132, \n",
    "         'r': 121915914, 'n': 1997759694, 'D': 295246830, 'y': 295246830, 'o': 67918650, 's': 156618468, \n",
    "         'S': 90662574, 'F': 492710238, '+': 65206260, 'i': 1414026, '/': 11547096, 'I': 23972994}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9486dfb1-4018-414c-914b-e56dc5ddfabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType, IntegerType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b86017-7e2f-4103-b591-0176c66a5401",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 01:26:39 WARN Utils: Your hostname, kanjur resolves to a loopback address: 127.0.1.1; using 10.119.2.14 instead (on interface eno3)\n",
      "24/12/24 01:26:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 01:26:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/24 01:26:40 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "# for 128 Gb and 32 Cores\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"16g\")\n",
    "#     .config(\"spark.executor.memory\", \"16g\")\n",
    "#     .config(\"spark.executor.instances\", \"4\")\n",
    "#     .config(\"spark.executor.cores\", \"4\")\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "#     .master(\"local[*]\")\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# for 256 Gb and 64 Cores\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka3\")\n",
    "    .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "    .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "    .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "    .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "    .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "    .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "    .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "    .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cc3b30-94e7-4237-9cbf-04a39f6b30f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293656924\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "|       id|                 bb1|                 bb2|                 bb3|            molecule|protein|  y|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "|101628733|[6, 0, 2, 1, 3, 6...|[1, 0, 0, 0, 2, 2...|[5, 0, 0, 0, 0, 0...|[7, 0, 2, 1, 4, 5...|      2|  0|\n",
      "|101298451|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[3, 0, 0, 0, 0, 3...|[7, 0, 2, 1, 1, 5...|      2|  0|\n",
      "|101166654|[6, 0, 2, 1, 2, 4...|[1, 0, 0, 0, 0, 1...|[4, 0, 0, 0, 1, 1...|[8, 0, 2, 1, 2, 3...|      1|  0|\n",
      "|107432121|[6, 0, 2, 1, 2, 4...|[2, 0, 0, 1, 0, 0...|[6, 0, 0, 0, 0, 0...|[8, 0, 1, 2, 1, 1...|      1|  0|\n",
      "|107230085|[6, 0, 2, 1, 2, 4...|[8, 0, 0, 0, 1, 1...|[1, 0, 0, 0, 0, 0...|[10, 0, 2, 1, 2, ...|      3|  0|\n",
      "|107926935|[6, 0, 2, 1, 2, 4...|[8, 0, 0, 0, 0, 2...|[9, 0, 2, 1, 0, 2...|[20, 0, 4, 2, 1, ...|      1|  0|\n",
      "|263512551|[9, 0, 1, 1, 3, 4...|[2, 0, 0, 0, 1, 1...|[1, 0, 0, 0, 0, 1...|[9, 0, 1, 1, 3, 3...|      1|  0|\n",
      "|125350291|[6, 0, 1, 1, 3, 6...|[4, 0, 0, 0, 0, 1...|[2, 0, 0, 0, 0, 0...|[9, 0, 1, 1, 2, 4...|      2|  0|\n",
      "|107896069|[6, 0, 2, 1, 2, 4...|[11, 0, 0, 1, 1, ...|[1, 0, 0, 0, 0, 1...|[14, 0, 2, 2, 2, ...|      2|  0|\n",
      "|107348768|[6, 0, 2, 1, 2, 4...|[10, 0, 0, 0, 0, ...|[5, 0, 0, 0, 0, 1...|[16, 0, 2, 1, 1, ...|      3|  0|\n",
      "|107505837|[6, 0, 2, 1, 2, 4...|[5, 0, 0, 0, 0, 1...|[1, 1, 0, 0, 0, 0...|[9, 1, 2, 1, 1, 2...|      1|  0|\n",
      "|125197338|[6, 0, 1, 1, 2, 4...|[0, 0, 0, 0, 1, 2...|[9, 0, 3, 4, 0, 1...|[12, 0, 4, 5, 2, ...|      1|  0|\n",
      "|263991526|[7, 0, 1, 1, 2, 4...|[2, 1, 0, 0, 0, 0...|[8, 0, 0, 0, 0, 2...|[14, 1, 1, 1, 1, ...|      2|  0|\n",
      "|263095646|[9, 0, 1, 1, 3, 4...|[7, 1, 0, 0, 1, 1...|[0, 0, 0, 1, 1, 1...|[12, 1, 2, 2, 4, ...|      3|  0|\n",
      "|107502947|[6, 0, 2, 1, 2, 4...|[6, 0, 0, 0, 0, 1...|[6, 0, 0, 0, 0, 2...|[15, 0, 2, 1, 1, ...|      3|  0|\n",
      "|107944856|[6, 0, 2, 1, 2, 4...|[7, 0, 0, 0, 0, 1...|[0, 0, 0, 0, 0, 0...|[10, 0, 2, 1, 1, ...|      3|  0|\n",
      "|101641006|[6, 0, 2, 1, 3, 6...|[4, 1, 0, 0, 0, 0...|[1, 0, 0, 0, 0, 1...|[7, 1, 2, 1, 2, 4...|      2|  0|\n",
      "|263465989|[9, 0, 1, 1, 3, 4...|[1, 0, 0, 0, 0, 0...|[1, 0, 0, 0, 0, 0...|[8, 0, 2, 1, 2, 1...|      2|  0|\n",
      "|125348188|[6, 0, 1, 1, 3, 6...|[8, 0, 0, 0, 0, 2...|[2, 0, 0, 0, 1, 3...|[13, 0, 1, 1, 3, ...|      2|  0|\n",
      "|101841227|[6, 0, 2, 1, 3, 6...|[7, 0, 0, 1, 0, 1...|[1, 0, 0, 0, 0, 0...|[9, 0, 1, 2, 2, 4...|      3|  0|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0 = spark.read.format('parquet').load('zero.parquet')\n",
    "\n",
    "print(df0.rdd.getNumPartitions())\n",
    "print(df0.count())\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222b3364-242b-44c6-9b2a-28dfeba0d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i in range(24):\n",
    "    cols.append(col('bb1').getItem(i).alias(f'a{i+1}'))\n",
    "    cols.append(col('bb2').getItem(i).alias(f'b{i+1}'))\n",
    "    cols.append(col('bb3').getItem(i).alias(f'c{i+1}'))\n",
    "    cols.append(col('molecule').getItem(i).alias(f'd{i+1}'))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('protein', IntegerType(), True),\n",
    "    *[StructField(f'a{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    *[StructField(f'b{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    *[StructField(f'c{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    *[StructField(f'd{i+1}', IntegerType(), True) for i in range(24)],\n",
    "    StructField('y', IntegerType(), True)\n",
    "])\n",
    "\n",
    "df0 = df0.select('id', 'protein', *cols, 'y')\n",
    "df0 = spark.createDataFrame(df0.rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4586f158-8029-4438-aab5-e42f1bcb0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 01:02:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id=101628733, protein=2, a1=6, a2=1, a3=5, a4=7, a5=0, a6=0, a7=0, a8=0, a9=2, a10=0, a11=0, a12=2, a13=1, a14=0, a15=0, a16=1, a17=3, a18=2, a19=0, a20=4, a21=6, a22=2, a23=0, a24=5, b1=2, b2=1, b3=1, b4=5, b5=18, b6=6, b7=5, b8=20, b9=2, b10=0, b11=0, b12=1, b13=0, b14=0, b15=2, b16=0, b17=0, b18=0, b19=2, b20=0, b21=0, b22=0, b23=0, b24=0, c1=0, c2=0, c3=0, c4=0, c5=0, c6=0, c7=1, c8=4, c9=0, c10=0, c11=0, c12=1, c13=0, c14=0, c15=0, c16=1, c17=0, c18=0, c19=0, c20=0, c21=0, c22=0, c23=0, c24=0, d1=0, d2=1, d3=0, d4=1, d5=0, d6=0, d7=0, d8=0, d9=1, d10=0, d11=0, d12=1, d13=0, d14=0, d15=0, d16=0, d17=0, d18=0, d19=0, d20=0, d21=0, d22=0, d23=0, d24=0, y=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a29900bc-373d-4bc1-8625-9ecab65802f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(df0.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60dc2913-614d-44fd-968d-db8bb71936cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.repartition(184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c85c528a-3842-4091-b6e2-26e23a9e418d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 64) / 92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239.499s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 11236 words\n",
      "[239.500s][warning][gc,alloc] Executor task launch worker for task 60.0 in stage 6.0 (TID 156): Retried waiting for GCLocker too often allocating 18757 words\n",
      "[239.502s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 29311 words\n",
      "[239.540s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 25693 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 30053 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 39366 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 32801 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 29311 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 59477 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 13118 words\n",
      "[239.765s][warning][gc,alloc] Executor task launch worker for task 46.0 in stage 6.0 (TID 142): Retried waiting for GCLocker too often allocating 34309 words\n",
      "[239.765s][warning][gc,alloc] Executor task launch worker for task 19.0 in stage 6.0 (TID 115): Retried waiting for GCLocker too often allocating 30331 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 53035 words\n",
      "[239.765s][warning][gc,alloc] Executor task launch worker for task 34.0 in stage 6.0 (TID 130): Retried waiting for GCLocker too often allocating 33631 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 17070 words\n",
      "[239.765s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 20905 words\n",
      "[239.801s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 13430 words\n",
      "[239.802s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 16438 words\n",
      "[239.802s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 9572 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:========================================================>(91 + 1) / 92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "print(df0.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69333ac-2261-43d8-95cc-35a0dca5005e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 01:27:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df0.write.format('parquet').mode('overwrite').option('header', True).save('zero_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce2221-b4a7-44dd-be1c-475d5a11960b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0_features = spark.read.format('parquet').load('zero_features.parquet')\n",
    "\n",
    "print(df0_features.rdd.getNumPartitions())\n",
    "print(df0_features.count())\n",
    "df0_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2da71-2923-4f5e-9136-18bc996e1da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eda0bb-6ba5-4069-bdc3-474069baac8f",
   "metadata": {},
   "source": [
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
