{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca027266-e09d-4d56-8456-36ff0fb3842f",
   "metadata": {},
   "source": [
    "## Model Creation on Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330bf9f2-a23a-411b-9dd3-3843f2f8d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 295246830\n",
    "one_len = 1589906\n",
    "zero_len = 293656924\n",
    "protein_map = {'BRD4': 1, 'HSA': 2, 'sEH': 3}\n",
    "vocab = {'C': 6825082866, '#': 81527490, '@': 511451694, 'H': 456489972, '=': 1406606874, 'O': 2554179786,\n",
    "         'N': 2469595230, 'c': 12257477022, '-': 438483636, '.': 216945504, 'l': 491088828, 'B': 123330132,\n",
    "         'r': 121915914, 'n': 1997759694, 'D': 295246830, 'y': 295246830, 'o': 67918650, 's': 156618468,\n",
    "         'S': 90662574, 'F': 492710238, '+': 65206260, 'i': 1414026, '/': 11547096, 'I': 23972994}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as sF\n",
    "from pyspark.sql.types import LongType, IntegerType, StructType, StructField\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7f7028-db0d-48ef-9902-8561b5902eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a564012-333f-4061-a649-f8814b1d3c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/27 03:20:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/27 03:20:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://passpoli.ieor.iitb.ac.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[64]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>leash belka3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efd00ee0620>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 256 Gb and 64 Cores\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     # .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.local.dir\", \"/scratch/23m1521/temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# SparkSession for 128 GB RAM and 64 cores\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimized Spark for 128GB RAM and 64 Cores\")\n",
    "    .config(\"spark.driver.memory\", \"64g\")  # 64GB for driver memory\n",
    "    .config(\"spark.executor.memory\", \"64g\")  # 64GB for executor memory\n",
    "    .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "    .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor (total = 64 cores)\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\")  # Driver result size limit\n",
    "    .config(\"spark.local.dir\", \"/scratch/23m1521/temp\")  # Temp directory with enough space\n",
    "    .config(\"spark.shuffle.file.buffer\", \"512k\")  # Increased shuffle buffer size\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "    .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "    .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n",
    "\n",
    "# SynapseML \n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.8\")\n",
    "#     .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"64g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"64g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"8\")  # Reduced number of executors\n",
    "#     .config(\"spark.executor.cores\", \"8\")  # Increased cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.7\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"1000\")  # Increase shuffle partitions\n",
    "#     .config(\"spark.ui.enabled\", \"true\")  # Enable Spark UI\n",
    "#     .master(\"local[8]\")  # Reduced number of cores for local mode\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc05b7f8-87f5-495d-a67d-f60e8b7ac823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_features = spark.read.format('parquet').load('zero_features.parquet')\n",
    "df1_features = spark.read.format('parquet').load('one_features.parquet')\n",
    "\n",
    "full_df = df0_features.union(df1_features).orderBy(sF.rand())\n",
    "\n",
    "# print(df0_features.rdd.getNumPartitions())\n",
    "# print(full_df.count())\n",
    "# df0_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d63a68-9b90-4a27-954c-4a5f0043db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df = full_df.sample(fraction=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25184b3-bb4a-4ed3-b17c-8bbd1c8b72b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "protein_ohe = OneHotEncoder(inputCol=\"protein\", outputCol=\"protein_onehot\")\n",
    "protein_ohe = protein_ohe.fit(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6124f07c-0615-4b2c-bfbc-99d7ee47df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = protein_ohe.transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752f135f-caca-4e7d-b041-616739fa68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = full_df.columns[-1:] + full_df.columns[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc838420-81eb-466e-aa9e-50e934da493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=features_cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96949d9-2025-4543-a9f7-06613d2b304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df2 = vectorAssembler.transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947127dc-3047-4e3c-a155-ce4ae137571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(full_df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00b080-dbdf-453f-ab26-479c4b5a5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df2 = full_df2.repartition(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f4ae2-5c06-4df1-9612-e1be86a55767",
   "metadata": {},
   "source": [
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2a3e89b-3f6c-483a-8423-2e5580838e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import os\n",
    "# import subprocess\n",
    "# import sys\n",
    "# from packaging import version\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# import horovod.spark.torch as hvd\n",
    "# from horovod.spark.common.backend import SparkBackend\n",
    "# from horovod.spark.common.store import Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43b68f74-03e0-44a6-a2b1-80264c93c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from pyspark.sql import SparkSession\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5ecf825-cece-4f45-a6c5-0fb3cdd20032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(total_steps, epoch, model, optimizer, loss, checkpoint_path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'total_steps': total_steps,\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved after {total_steps} steps at epoch {epoch}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path=\"checkpoint.pth\"):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        total_steps = checkpoint['total_steps']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        return total_steps, start_epoch, loss\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return 0, 0, None\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(300, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bfbbf6c5-8416-467c-8a41-e9a79bc26777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = full_df2.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e503c7c-db0f-48a7-9bf4-c7a7305dec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 99\n",
    "multi_GPU = True\n",
    "\n",
    "if multi_GPU:\n",
    "    model = torch.nn.DataParallel(BinaryClassifier(input_dim), device_ids=[0, 1]).to(device)\n",
    "else:\n",
    "    model = BinaryClassifier(input_dim).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "919ec98b-4fa8-43b7-b0af-cdde94940a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(1024*1)\n",
    "checkpoint_path = \"checkpoints/_2_PyTorch.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3d13a97-b67f-43b5-a5ce-b224e98273e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting from scratch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a96a903b974289b61c4fa036b24708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b88b3557b649eeba04473cc3bac896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                                      | 0/288326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _local_iterator_from_socket.<locals>.PyLocalIterable.__del__ at 0x7efafd6f4e00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/rdd.py\", line 308, in __del__\n",
      "    for _ in self._read_iter:\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2208, in <lambda>\n",
      "    return lambda *a: dataType.fromInternal(a)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1088, in fromInternal\n",
      "    f.fromInternal(v) if c else v\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/sql/types.py\", line 720, in fromInternal\n",
      "    return self.dataType.fromInternal(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/sql/types.py\", line 1153, in fromInternal\n",
      "    return self.deserialize(v)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/ml/linalg/__init__.py\", line 205, in deserialize\n",
      "    return SparseVector(cast(int, datum[1]), cast(List[int], datum[2]), datum[3])\n",
      "                                                  ~~~~^^^^^\n",
      "  File \"/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/typing.py\", line 395, in inner\n",
      "    return _caches[func](*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "24/12/27 04:34:47 ERROR Utils: Aborting task\n",
      "java.net.SocketException: Connection reset by peer\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:425)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:445)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:831)\n",
      "\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1035)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n",
      "\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$2(PythonRDD.scala:263)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1(PythonRDD.scala:275)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1$adapted(PythonRDD.scala:226)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)\n",
      "24/12/27 04:34:47 WARN Utils: Suppressing exception in catch: Broken pipe\n",
      "java.net.SocketException: Broken pipe\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:425)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:445)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:831)\n",
      "\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1035)\n",
      "\tat java.base/java.io.DataOutputStream.writeInt(DataOutputStream.java:208)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$7(PythonRDD.scala:274)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1(PythonRDD.scala:275)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1$adapted(PythonRDD.scala:226)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m batch_labels\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_features) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m---> 24\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch_labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_partition(features, labels)\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/ml/linalg/__init__.py:843\u001b[0m, in \u001b[0;36mSparseVector.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (inds\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (index \u001b[38;5;241m>\u001b[39m inds\u001b[38;5;241m.\u001b[39mitem(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfloat64(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m--> 843\u001b[0m insert_index \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43minds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m row_ind \u001b[38;5;241m=\u001b[39m inds[insert_index]\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_ind \u001b[38;5;241m==\u001b[39m index:\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/numpy/core/fromnumeric.py:1400\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \n\u001b[1;32m   1399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_env2/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/27 04:34:48 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /home/23m1521/ashish/kaggle/temp3/blockmgr-dc500c77-b700-4697-9d12-431eaf065634. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /home/23m1521/ashish/kaggle/temp3/blockmgr-dc500c77-b700-4697-9d12-431eaf065634\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "def train_partition(features, labels):\n",
    "    output = model(features)\n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    \n",
    "total_steps = 0\n",
    "total_steps, start_epoch, prev_loss = load_checkpoint(checkpoint_path)\n",
    "\n",
    "for epoch in trange(start_epoch, 10, desc='Epoch', dynamic_ncols=True):\n",
    "    batch_features = []\n",
    "    batch_labels = []\n",
    "    epoch_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    with tqdm(total=int(train_len / batch_size), desc=\"Training\", dynamic_ncols=True) as pbar:\n",
    "        for row in full_df2.rdd.toLocalIterator():\n",
    "            batch_features.append(row['features'])\n",
    "            batch_labels.append(row['y'])\n",
    "\n",
    "            if len(batch_features) == batch_size:\n",
    "                features = torch.tensor(batch_features, dtype=torch.float32).to(device)\n",
    "                labels = torch.tensor(batch_labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "                loss = train_partition(features, labels)\n",
    "                epoch_loss += loss\n",
    "                steps += 1\n",
    "\n",
    "                batch_features = []\n",
    "                batch_labels = []\n",
    "                \n",
    "                if steps % 10000 == 0:\n",
    "                    save_checkpoint(total_steps+steps, epoch, model, \n",
    "                                    optimizer, epoch_loss, checkpoint_path)\n",
    "                    \n",
    "                pbar.set_description(f\"Total Steps: {total_steps+steps}\")\n",
    "                pbar.set_postfix_str(f\"Eloss: {epoch_loss / steps} | BLoss: {loss}\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "    epoch_loss /= steps\n",
    "    total_steps += steps\n",
    "    print(f\"Epoch: {epoch + 1} | Loss: {epoch_loss}\")\n",
    "\n",
    "    save_checkpoint(total_steps, epoch, model, optimizer, epoch_loss, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30fc485-4316-4f61-b6b7-83e4fad4cc7d",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cffad-2c8d-4bc4-a815-53148b5124c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984a578-a8d8-4ce8-8b2b-c025a1aedf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
