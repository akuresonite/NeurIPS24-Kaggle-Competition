{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_len = 295246830\n",
    "one_len = 1589906\n",
    "zero_len = 293656924\n",
    "protein_map = {'BRD4': 1, 'HSA': 2, 'sEH': 3}\n",
    "vocab = {'C': 6825082866, '#': 81527490, '@': 511451694, 'H': 456489972, '=': 1406606874, 'O': 2554179786,\n",
    "         'N': 2469595230, 'c': 12257477022, '-': 438483636, '.': 216945504, 'l': 491088828, 'B': 123330132,\n",
    "         'r': 121915914, 'n': 1997759694, 'D': 295246830, 'y': 295246830, 'o': 67918650, 's': 156618468,\n",
    "         'S': 90662574, 'F': 492710238, '+': 65206260, 'i': 1414026, '/': 11547096, 'I': 23972994}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, IntegerType, StructType, StructField, ArrayType, DoubleType, StringType\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StringIndexerModel, OneHotEncoderModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# from xgboost.spark import SparkXGBClassifier\n",
    "# from functools import wraps\n",
    "# import xgboost as xgb\n",
    "\n",
    "# from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import Parallel, delayed\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import Descriptors, Lipinski, rdmolops, AllChem, rdchem, rdEHTTools, rdMolDescriptors\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "# from padelpy import from_smiles\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 03:35:32 WARN Utils: Your hostname, kanjur resolves to a loopback address: 127.0.1.1; using 10.119.2.14 instead (on interface eno3)\n",
      "25/01/03 03:35:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://mmlspark.azureedge.net/maven added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/23m1521/.ivy2/cache\n",
      "The jars for the packages stored in: /home/23m1521/.ivy2/jars\n",
      "com.microsoft.azure#synapseml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-798cb5a4-399c-4508-ae50-3d276eff8f12;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#synapseml_2.12;1.0.8 in spark-list\n",
      "\tfound com.microsoft.azure#synapseml-core_2.12;1.0.8 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.4.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound org.scalactic#scalactic_2.12;3.2.14 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.15 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.5 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.1.3 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.1.3 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.1.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.10 in central\n",
      "\tfound org.testng#testng;6.8.8 in central\n",
      "\tfound org.beanshell#bsh;2.0b4 in central\n",
      "\tfound com.beust#jcommander;1.27 in central\n",
      "\tfound org.scalanlp#breeze_2.12;2.1.0 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;2.1.0 in central\n",
      "\tfound org.typelevel#spire_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-macros_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#algebra_2.12;2.0.1 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.1.1 in central\n",
      "\tfound org.typelevel#spire-platform_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-util_2.12;0.17.0 in central\n",
      "\tfound dev.ludovic.netlib#blas;3.0.1 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound dev.ludovic.netlib#lapack;3.0.1 in central\n",
      "\tfound dev.ludovic.netlib#arpack;3.0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.wendykierp#JTransforms;3.1 in central\n",
      "\tfound pl.edu.icm#JLargeArrays;1.5 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.7.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-deep-learning_2.12;1.0.8 in central\n",
      "\tfound com.microsoft.azure#synapseml-opencv_2.12;1.0.8 in central\n",
      "\tfound org.openpnp#opencv;3.2.0-1 in central\n",
      "\tfound com.microsoft.azure#onnx-protobuf_2.12;0.9.3 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-cognitive_2.12;1.0.8 in central\n",
      "\tfound com.microsoft.cognitiveservices.speech#client-sdk;1.24.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-vw_2.12;1.0.8 in central\n",
      "\tfound com.github.vowpalwabbit#vw-jni;9.3.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-lightgbm_2.12;1.0.8 in central\n",
      "\tfound com.microsoft.ml.lightgbm#lightgbmlib;3.3.510 in central\n",
      ":: resolution report :: resolve 1686ms :: artifacts dl 98ms\n",
      "\t:: modules in use:\n",
      "\tcom.beust#jcommander;1.27 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.10 from central in [default]\n",
      "\tcom.github.vowpalwabbit#vw-jni;9.3.0 from central in [default]\n",
      "\tcom.github.wendykierp#JTransforms;3.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from central in [default]\n",
      "\tcom.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 from central in [default]\n",
      "\tcom.microsoft.azure#onnx-protobuf_2.12;0.9.3 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-cognitive_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-core_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-deep-learning_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-lightgbm_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-opencv_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-vw_2.12;1.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml_2.12;1.0.8 from spark-list in [default]\n",
      "\tcom.microsoft.cognitiveservices.speech#client-sdk;1.24.1 from central in [default]\n",
      "\tcom.microsoft.ml.lightgbm#lightgbmlib;3.3.510 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdev.ludovic.netlib#arpack;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#blas;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#lapack;3.0.1 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.5 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.1.3 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.1.3 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.4.1 from central in [default]\n",
      "\torg.beanshell#bsh;2.0b4 from central in [default]\n",
      "\torg.openpnp#opencv;3.2.0-1 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.15 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.7.0 from central in [default]\n",
      "\torg.scalactic#scalactic_2.12;3.2.14 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;2.1.0 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;2.1.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\torg.testng#testng;6.8.8 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.typelevel#algebra_2.12;2.0.1 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.1.1 from central in [default]\n",
      "\torg.typelevel#spire-macros_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-platform_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-util_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire_2.12;0.17.0 from central in [default]\n",
      "\tpl.edu.icm#JLargeArrays;1.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.2.0 by [org.scala-lang.modules#scala-collection-compat_2.12;2.7.0] in [default]\n",
      "\torg.apache.commons#commons-math3;3.5 by [org.apache.commons#commons-math3;3.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.25] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   55  |   0   |   0   |   4   ||   51  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-798cb5a4-399c-4508-ae50-3d276eff8f12\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 51 already retrieved (0kB/38ms)\n",
      "25/01/03 03:35:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/03 03:35:35 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kanjur.ieor.iitb.ac.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>leash belka346765</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f72fe72eb70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # for 256 Gb and 64 Cores\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     # .config(\"spark.local.dir\", \"/scratch/23m1521/temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"leash belka346765\")\n",
    "    .config(\"spark.driver.memory\", \"64g\")  # Increased driver memory for large jobs\n",
    "    .config(\"spark.executor.memory\", \"64g\")  # Increased executor memory\n",
    "    .config(\"spark.executor.instances\", \"32\")  # 32 executors\n",
    "    .config(\"spark.executor.cores\", \"2\")  # 2 cores per executor\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\")  # Driver result size limit\n",
    "    .config(\"spark.local.dir\", \"temp\")  # Ensure high-speed storage\n",
    "    .config(\"spark.shuffle.file.buffer\", \"1024k\")  # Larger shuffle buffer for better IO\n",
    "    .config(\"spark.memory.fraction\", \"0.85\")  # Increased memory for tasks\n",
    "    .config(\"spark.shuffle.memoryFraction\", \"0.7\")  # Increased shuffle memory\n",
    "    .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.8\")\n",
    "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "    .master(\"local[*]\")  # Use all 64 cores on the machine\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark\n",
    "\n",
    "# SparkSession for 128 GB RAM and 64 cores\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"Optimized Spark for 128GB RAM and 64 Cores\")\n",
    "#     .config(\"spark.driver.memory\", \"64g\")  # 64GB for driver memory\n",
    "#     .config(\"spark.executor.memory\", \"64g\")  # 64GB for executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor (total = 64 cores)\n",
    "#     .config(\"spark.driver.maxResultSize\", \"8g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Temp directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"512k\")  # Increased shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# SynapseML \n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"48g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"48g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"16\")  # 16 executors\n",
    "#     .config(\"spark.executor.cores\", \"4\")  # 4 cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.6\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx48g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.8\")\n",
    "#     .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "#     .master(\"local[64]\")  # Use all 64 cores on the machine\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"leash belka3\")\n",
    "#     .config(\"spark.driver.memory\", \"64g\")  # Increased driver memory\n",
    "#     .config(\"spark.executor.memory\", \"64g\")  # Increased executor memory\n",
    "#     .config(\"spark.executor.instances\", \"8\")  # Reduced number of executors\n",
    "#     .config(\"spark.executor.cores\", \"8\")  # Increased cores per executor\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\")  # Driver result size limit\n",
    "#     .config(\"spark.local.dir\", \"temp\")  # Specify a directory with enough space\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"128k\")  # Shuffle buffer size\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\")  # Memory fraction for tasks\n",
    "#     .config(\"spark.shuffle.memoryFraction\", \"0.7\")  # Shuffle memory fraction\n",
    "#     .config(\"spark.executor.javaOptions\", \"-Xmx64g\")  # JVM heap size for executors\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"1000\")  # Increase shuffle partitions\n",
    "#     .config(\"spark.ui.enabled\", \"true\")  # Enable Spark UI\n",
    "#     .master(\"local[8]\")  # Reduced number of cores for local mode\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "datadir = \"/home/23m1521/ashish/kaggle/full_feat_tok_df_vectors.parquet\"\n",
    "chunks_path = sorted([os.path.join(datadir, i) for i in os.listdir(datadir) if i.endswith(\".parquet\")])\n",
    "total_chunks = len(chunks_path)\n",
    "print(total_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476728\n",
      "+----+--------------------+-----+\n",
      "|  id|             vectors|binds|\n",
      "+----+--------------------+-----+\n",
      "|  65|(190,[0,1,2,3,4,5...|    0|\n",
      "| 191|(190,[0,1,2,3,4,5...|    0|\n",
      "| 418|(190,[0,1,2,3,4,5...|    0|\n",
      "| 541|(190,[0,1,2,3,4,5...|    0|\n",
      "| 558|(190,[0,1,2,3,4,5...|    0|\n",
      "|1010|(190,[0,1,2,3,4,5...|    0|\n",
      "|1224|(190,[0,1,2,3,4,5...|    0|\n",
      "|1258|(190,[0,1,2,3,4,5...|    0|\n",
      "|1277|(190,[0,1,2,3,4,5...|    0|\n",
      "|1360|(190,[0,1,2,3,4,5...|    0|\n",
      "|1840|(190,[0,1,2,3,4,5...|    0|\n",
      "|2173|(190,[0,1,2,3,4,5...|    0|\n",
      "|2812|(190,[0,1,2,3,4,5...|    0|\n",
      "|2895|(190,[0,1,2,3,4,5...|    0|\n",
      "|2906|(190,[0,1,2,3,4,5...|    0|\n",
      "|2941|(190,[0,1,2,3,4,5...|    0|\n",
      "|3009|(190,[0,1,2,3,4,5...|    0|\n",
      "|3015|(190,[0,1,2,3,4,5...|    0|\n",
      "|3061|(190,[0,1,2,3,4,5...|    0|\n",
      "|3106|(190,[0,1,2,3,4,5...|    0|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_feat_tok_df_vectors = spark.read.format('parquet').load(chunks_path[1])\n",
    "print(full_feat_tok_df_vectors.count())\n",
    "full_feat_tok_df_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: 1468867, 1: 7861}, 0.00535174389512461)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = dict(full_feat_tok_df_vectors.groupBy(\"binds\").count().collect())\n",
    "contamination = class_counts[1]/class_counts[0]\n",
    "class_counts, contamination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Isolation Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def make_dataset2(df, test=False):\n",
    "    df = df.toPandas()\n",
    "    df.vectors = df.vectors.map(lambda x: x.toArray())\n",
    "    if test == True:\n",
    "        return df.id.values, np.array([i for i in df.vectors.values]), None, None\n",
    "    return df.id.values, np.array([i for i in df.vectors.values]), df.binds.values\n",
    "\n",
    "ids, X, y = make_dataset2(full_feat_tok_df_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(bootstrap=True, contamination=0.00535174389512461,\n",
       "                max_features=0.7, max_samples=0.5, n_estimators=500, n_jobs=-1,\n",
       "                random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;IsolationForest<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.IsolationForest.html\">?<span>Documentation for IsolationForest</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>IsolationForest(bootstrap=True, contamination=0.00535174389512461,\n",
       "                max_features=0.7, max_samples=0.5, n_estimators=500, n_jobs=-1,\n",
       "                random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(bootstrap=True, contamination=0.00535174389512461,\n",
       "                max_features=0.7, max_samples=0.5, n_estimators=500, n_jobs=-1,\n",
       "                random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "isolation_forest = IsolationForest(\n",
    "    n_estimators=500,\n",
    "    max_samples=0.5,  # Or \"auto\" or a specific integer\n",
    "    contamination=contamination,  # Or \"auto\" for default behavior\n",
    "    max_features=0.7,  # Or an integer for a fixed number of features\n",
    "    bootstrap=True,  # Whether to use bootstrapping\n",
    "    n_jobs=-1,      # Use all available processors\n",
    "    random_state=42,  # For reproducibility\n",
    "    verbose=0,       # Verbosity level\n",
    "    warm_start=False # Reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.\n",
    ")\n",
    "\n",
    "isolation_forest.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier indices: [    231    1523    1546 ... 1476708 1476709 1476712]\n",
      "[0.08211198 0.07574384 0.0801966  ... 0.02335361 0.05317712 0.03663002]\n"
     ]
    }
   ],
   "source": [
    "predictions = isolation_forest.predict(X) # 1 for inliers, -1 for outliers\n",
    "scores = isolation_forest.decision_function(X) # Anomaly score for each sample\n",
    "outlier_indices = np.where(predictions == -1)[0]\n",
    "print(\"Outlier indices:\", outlier_indices)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00505229 0.99494771]\n",
      " [0.00484956 0.99515044]\n",
      " [0.00498959 0.99501041]\n",
      " ...\n",
      " [0.00367181 0.99632819]\n",
      " [0.00424839 0.99575161]\n",
      " [0.00390274 0.99609726]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def anomaly_scores_to_probabilities_calibration(y_true, scores, higher_is_anomalous=True, method='isotonic'):\n",
    "    \"\"\"Calibrates anomaly scores to probabilities using isotonic regression or Platt scaling.\"\"\"\n",
    "    if higher_is_anomalous:\n",
    "        scores = -scores #Isotonic regression work with increasing values\n",
    "    if method == 'isotonic':\n",
    "        ir = IsotonicRegression()\n",
    "        calibrated_scores = ir.fit_transform(scores, y_true)\n",
    "    elif method == 'platt':\n",
    "        calibrated_scores = CalibratedClassifierCV(\n",
    "            estimator = LogisticRegression(n_jobs=-1),\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "            ).fit(scores.reshape(-1, 1), y_true).predict_proba(scores.reshape(-1,1))[:,1]\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'isotonic' or 'platt'\")\n",
    "    if higher_is_anomalous:\n",
    "        probabilities_1 = calibrated_scores\n",
    "        probabilities_0 = 1- calibrated_scores\n",
    "    else:\n",
    "        probabilities_0 = calibrated_scores\n",
    "        probabilities_1 = 1- calibrated_scores\n",
    "    return np.column_stack((probabilities_0, probabilities_1))\n",
    "\n",
    "\n",
    "probabilities_normal = anomaly_scores_to_probabilities_calibration(y, scores, \n",
    "                                                                   higher_is_anomalous=False, method='platt'\n",
    "                                                                   )\n",
    "print(probabilities_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99494771, 0.99515044, 0.99501041, ..., 0.99632819, 0.99575161,\n",
       "       0.99609726])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_normal[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary AUC-PR: 0.005970386939440327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00466528435253541, 0.005970386939440327)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def calculate_auc_pr(y_true, y_scores):\n",
    "    try:\n",
    "        # Handle multiclass case using one-vs-rest binarization\n",
    "        if len(np.unique(y_true)) > 2:\n",
    "            lb = LabelBinarizer()\n",
    "            y_true_bin = lb.fit_transform(y_true)\n",
    "            if y_true_bin.shape[1] == 1:\n",
    "                y_true_bin = np.hstack([1 - y_true_bin, y_true_bin]) #handle the case where labelbinarizer return only one column\n",
    "            auc_pr_scores = []\n",
    "            for i in range(y_true_bin.shape[1]):\n",
    "                precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "                auc_pr = auc(recall, precision)\n",
    "                auc_pr_scores.append(auc_pr)\n",
    "            return np.mean(auc_pr_scores) # Return the mean AUC-PR over all classes.\n",
    "        else: #binary case\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "            return auc(recall, precision)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error calculating AUC-PR: {e}\")\n",
    "        return None  # Handle cases like only one class being present\n",
    "\n",
    "\n",
    "\n",
    "auc_pr_binary = calculate_auc_pr(y, scores)\n",
    "print(f\"Binary AUC-PR: {auc_pr_binary}\")\n",
    "calculate_auc_pr(y, probabilities_normal[:,1]), calculate_auc_pr(y, probabilities_normal[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.isolationforest import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolationForest = IsolationForest(\n",
    "    bootstrap=False,\n",
    "    contamination=contamination,\n",
    "    contaminationError=0.01 * contamination,\n",
    "    featuresCol=\"vectors\",\n",
    "    maxFeatures=1.0,\n",
    "    maxSamples=0.2,\n",
    "    numEstimators=1000,\n",
    "    predictionCol=\"predictedLabel\",\n",
    "    randomSeed=42,\n",
    "    scoreCol=\"outlierScore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = isolationForest.fit(full_feat_tok_df_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feat_tok_df_vectors = model.transform(full_feat_tok_df_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 23:47:30 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "25/01/02 23:47:33 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "25/01/02 23:47:37 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "[Stage 27:=======================================>                (14 + 6) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+-------------------+--------------+\n",
      "|  id|             vectors|binds|       outlierScore|predictedLabel|\n",
      "+----+--------------------+-----+-------------------+--------------+\n",
      "|  65|(190,[0,1,2,3,4,5...|    0|0.38952457174064203|           0.0|\n",
      "| 191|(190,[0,1,2,3,4,5...|    0| 0.3935112108060045|           0.0|\n",
      "| 418|(190,[0,1,2,3,4,5...|    0|0.40572750302732014|           0.0|\n",
      "| 541|(190,[0,1,2,3,4,5...|    0|0.39243424010460193|           0.0|\n",
      "| 558|(190,[0,1,2,3,4,5...|    0| 0.3927223271193177|           0.0|\n",
      "|1010|(190,[0,1,2,3,4,5...|    0| 0.3851334912164859|           0.0|\n",
      "|1224|(190,[0,1,2,3,4,5...|    0| 0.4213978244924502|           0.0|\n",
      "|1258|(190,[0,1,2,3,4,5...|    0|0.40407570713910124|           0.0|\n",
      "|1277|(190,[0,1,2,3,4,5...|    0|0.42491467645957937|           0.0|\n",
      "|1360|(190,[0,1,2,3,4,5...|    0| 0.4302462718482658|           0.0|\n",
      "|1840|(190,[0,1,2,3,4,5...|    0|0.39414203209719023|           0.0|\n",
      "|2173|(190,[0,1,2,3,4,5...|    0|0.39923685452078933|           0.0|\n",
      "|2812|(190,[0,1,2,3,4,5...|    0| 0.4004141088380974|           0.0|\n",
      "|2895|(190,[0,1,2,3,4,5...|    0|0.40132796113812996|           0.0|\n",
      "|2906|(190,[0,1,2,3,4,5...|    0|0.38870841213083696|           0.0|\n",
      "|2941|(190,[0,1,2,3,4,5...|    0|0.38566387514490186|           0.0|\n",
      "|3009|(190,[0,1,2,3,4,5...|    0| 0.4002665722387892|           0.0|\n",
      "|3015|(190,[0,1,2,3,4,5...|    0| 0.3925485607913668|           0.0|\n",
      "|3061|(190,[0,1,2,3,4,5...|    0|0.38762796215529943|           0.0|\n",
      "|3106|(190,[0,1,2,3,4,5...|    0|0.39922135009770565|           0.0|\n",
      "+----+--------------------+-----+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_feat_tok_df_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 23:47:51 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "25/01/02 23:51:09 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(predictedLabel=0.0, count=1468796), Row(predictedLabel=1.0, count=7932)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_feat_tok_df_vectors.groupBy(\"predictedLabel\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 23:51:15 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC (approx MAP): 0.004571768784415767\n"
     ]
    }
   ],
   "source": [
    "pr_auc_evaluator = BinaryClassificationEvaluator(labelCol=\"binds\", \n",
    "                                                 rawPredictionCol=\"outlierScore\", \n",
    "                                                 metricName=\"areaUnderPR\")\n",
    "pr_auc = pr_auc_evaluator.evaluate(full_feat_tok_df_vectors)\n",
    "print(f\"PR AUC (approx MAP): {pr_auc}\")\n",
    "\n",
    "# pr_auc = pr_auc_evaluator.evaluate(full_predictions)\n",
    "# print(f\"PR AUC (approx MAP): {pr_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 23:57:11 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC (approx MAP): 0.00436029818974985\n"
     ]
    }
   ],
   "source": [
    "pr_auc_evaluator = BinaryClassificationEvaluator(labelCol=\"binds\", \n",
    "                                                 rawPredictionCol=\"predictedLabel\", \n",
    "                                                 metricName=\"areaUnderPR\")\n",
    "pr_auc = pr_auc_evaluator.evaluate(full_feat_tok_df_vectors)\n",
    "print(f\"PR AUC (approx MAP): {pr_auc}\")\n",
    "\n",
    "# pr_auc = pr_auc_evaluator.evaluate(full_predictions)\n",
    "# print(f\"PR AUC (approx MAP): {pr_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 23:56:15 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "25/01/02 23:56:18 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "25/01/02 23:56:22 WARN DAGScheduler: Broadcasting large task binary with size 81.0 MiB\n",
      "[Stage 47:==================================================>     (18 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+-------------------+--------------+\n",
      "|     id|             vectors|binds|       outlierScore|predictedLabel|\n",
      "+-------+--------------------+-----+-------------------+--------------+\n",
      "|  43442|(190,[0,1,2,3,4,5...|    1|0.42633382085345667|           0.0|\n",
      "|  69784|(190,[0,1,2,3,4,5...|    1|0.39403948946242146|           0.0|\n",
      "| 465211|(190,[0,1,2,3,4,5...|    1| 0.3849148945590878|           0.0|\n",
      "| 498101|(190,[0,1,2,3,4,5...|    1| 0.3895195185317123|           0.0|\n",
      "| 509867|(190,[0,1,2,3,4,5...|    1| 0.3785149765255084|           0.0|\n",
      "| 510692|(190,[0,1,2,3,4,5...|    1|0.38751757206339965|           0.0|\n",
      "| 510851|(190,[0,1,2,3,4,5...|    1| 0.3787319101572828|           0.0|\n",
      "| 532509|(190,[0,1,2,3,4,5...|    1| 0.3971351162532173|           0.0|\n",
      "| 797769|(190,[0,1,2,3,4,5...|    1|0.37282339852420276|           0.0|\n",
      "| 930648|(190,[0,1,2,3,4,5...|    1| 0.4288135725238536|           0.0|\n",
      "| 967915|(190,[0,1,2,3,4,5...|    1| 0.4121725667361934|           0.0|\n",
      "| 986287|(190,[0,1,2,3,4,5...|    1| 0.3870397060826099|           0.0|\n",
      "|1002688|(190,[0,1,2,3,4,5...|    1|0.38341473614201244|           0.0|\n",
      "|1285334|(190,[0,1,2,3,4,5...|    1| 0.3917485939925907|           0.0|\n",
      "|1358094|(190,[0,1,2,3,4,5...|    1|0.38450024897651464|           0.0|\n",
      "|1548020|(190,[0,1,2,3,4,5...|    1| 0.3929574038390626|           0.0|\n",
      "|1599028|(190,[0,1,2,3,4,5...|    1|0.41900104697855034|           0.0|\n",
      "|1741429|(190,[0,1,2,3,4,5...|    1|0.40767484000769905|           0.0|\n",
      "|2065952|(190,[0,1,2,3,4,5...|    1| 0.3958761298874134|           0.0|\n",
      "|2231019|(190,[0,1,2,3,4,5...|    1|0.41085497742464244|           0.0|\n",
      "+-------+--------------------+-----+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_feat_tok_df_vectors.where(\"binds == 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_chunk(path):\n",
    "    return spark.read.format('parquet').load(path)\n",
    "\n",
    "def add_sample_weights(df):\n",
    "    class_counts = df.groupBy(\"binds\").count().collect()\n",
    "    total_count = sum(row[\"count\"] for row in class_counts)\n",
    "    class_weights = {row[\"binds\"]: total_count / (2 * row[\"count\"]) for row in class_counts}\n",
    "    return class_counts, df.withColumn(\"sample_weights\", when(col(\"binds\") == 0, class_weights[0]).when(col(\"binds\") == 1, class_weights[1]))\n",
    "\n",
    "def make_dataset(df, chunk_df_count):\n",
    "    def process_row(row):\n",
    "        return (row['vectors'].toArray(), row['binds'], row['sample_weights'])\n",
    "    features, labels, weights = [], [], []\n",
    "    for feature, label, weight in tqdm(df.rdd.map(process_row).toLocalIterator(), total=chunk_df_count):\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "    return features, labels, weights\n",
    "\n",
    "def make_dataset2(df, test=False):\n",
    "    df = df.toPandas()\n",
    "    df.vectors = df.vectors.map(lambda x: x.toArray())\n",
    "    if test == True:\n",
    "        return df.id.values, np.array([i for i in df.vectors.values]), None, None\n",
    "    return df.id.values, np.array([i for i in df.vectors.values]), df.binds.values, df.sample_weights.values\n",
    "\n",
    "def save_checkpoint(model, params, i, evals_result,  path, save_name):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    model.save_model(os.path.join(path, f\"{save_name}.json\"))\n",
    "    joblib.dump({\"params\": params, 'i': i, \"evals_result\": evals_result}, os.path.join(path, f\"{save_name}_params.joblib\"))\n",
    "    print(\"Model saved at\", path)\n",
    "\n",
    "def load_checkpoint(path, save_name):\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(os.path.join(path, f\"{save_name}.json\"))\n",
    "    ckpt = joblib.load(os.path.join(path, f\"{save_name}_params.joblib\"))\n",
    "    params, i = ckpt['params'], ckpt['i']\n",
    "    print(\"Model loaded from\", path)\n",
    "    return model, params, i\n",
    "\n",
    "def train_IsolationForest(X, xgb_model=None):\n",
    "    model = IsolationForest(\n",
    "        n_estimators=100,\n",
    "        contamination=contamination,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        warm_start=True\n",
    "        ).fit(X)\n",
    "    return bst, evals_result, best_params1\n",
    "\n",
    "def delete_df_chunk(df):\n",
    "    df.unpersist()\n",
    "    del df\n",
    "\n",
    "def spark_suppress_logs(level=\"ERROR\", reset_level=\"INFO\"):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            spark.sparkContext.setLogLevel(level)\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                spark.sparkContext.setLogLevel(reset_level)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@spark_suppress_logs()\n",
    "def incrementally_train():\n",
    "    with tqdm(total=total_chunks, dynamic_ncols=True) as pbar1:\n",
    "        sample_count = 0\n",
    "        xgb_model = None\n",
    "        ckpt_dir = \"Incrementally_train_XGB_ckpt\"\n",
    "        \n",
    "        for i, chunk_path in enumerate(chunks_path):\n",
    "            # if i > 1:\n",
    "            #     break\n",
    "        \n",
    "            pbar1.set_description(f\"Chunk: {i+1}\")\n",
    "            \n",
    "            \n",
    "            # --- Load chunk --------------------------------------\n",
    "            chunk_df = load_df_chunk(chunk_path)\n",
    "            chunk_df = add_sample_weights(chunk_df)\n",
    "            chunk_df = chunk_df.repartition(1)\n",
    "            chunk_df_count = chunk_df.count()\n",
    "            \n",
    "            # --- Getting Dataset ---------------------------------\n",
    "            _, features, labels, weights = make_dataset2(chunk_df)\n",
    "            dtrain = xgb.DMatrix(data=features, label=labels, weight=weights, nthread=25)\n",
    "            \n",
    "            # # --- Train ------------------------------------------\n",
    "            if xgb_model is None:\n",
    "                xgb_model, evals_result, params = train_xgb(dtrain)\n",
    "            else:\n",
    "                xgb_model, evals_result, params = train_xgb(dtrain, xgb_model)\n",
    "            save_checkpoint(xgb_model, params, i, evals_result, ckpt_dir, f\"_{i+1}_ckpt\")\n",
    "            \n",
    "            # # --- Model Evaluation -------------------------------\n",
    "            loss = np.mean(evals_result['train']['logloss'])\n",
    "            aucpr = np.mean(evals_result['train']['aucpr'])\n",
    "            print(f\"Chunk {i+1} trained. Loss: {loss}, AUCPR: {aucpr}\")\n",
    "            \n",
    "            \n",
    "            # --- Clean up ----------------------------------------\n",
    "            # delete_df_chunk(chunk_df)\n",
    "            del chunk_df, features, labels, weights, dtrain, _\n",
    "            sample_used = sample_count + chunk_df_count\n",
    "            sample_used_percentage = (sample_used / train_len) * 100\n",
    "            remaining_samples = train_len - sample_used\n",
    "            remaining_samples_percentage = (remaining_samples / train_len) * 100\n",
    "            pbar1.set_postfix_str(\n",
    "                f\"{sample_used} ({sample_used_percentage:.2f}%) samples used,\" \n",
    "                f\"{remaining_samples} ({remaining_samples_percentage:.2f}%) samples remaining\"\n",
    "            )\n",
    "            pbar1.update(1)\n",
    "            sample_count += chunk_df_count\n",
    "            \n",
    "        # --- Save final model -----------------------------------\n",
    "        save_checkpoint(xgb_model, params, i, evals_result, ckpt_dir, f\"Final_ckpt\")\n",
    "\n",
    "\n",
    "@spark_suppress_logs()\n",
    "def incrementally_inference(model, chunks_path, save_dir):\n",
    "    with tqdm(total=total_chunks, dynamic_ncols=True) as pbar1:\n",
    "        sample_count = 0\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # id_array = np.array([], dtype=np.uint64)\n",
    "        # preds_array = np.array([], dtype=np.float128)\n",
    "        \n",
    "        for i, chunk_path in enumerate(chunks_path):\n",
    "            pbar1.set_description(f\"Chunk: {i+1}\")\n",
    "            \n",
    "            \n",
    "            # --- Load chunk --------------------------------------\n",
    "            chunk_df = load_df_chunk(chunk_path)\n",
    "            # chunk_df = add_sample_weights(chunk_df)\n",
    "            chunk_df = chunk_df.repartition(1)\n",
    "            chunk_df_count = chunk_df.count()\n",
    "            \n",
    "            # --- Getting Dataset ---------------------------------\n",
    "            ids, features, labels, weights = make_dataset2(chunk_df, test=True)\n",
    "            dtest = xgb.DMatrix(data=features, nthread=25)\n",
    "            \n",
    "            # # --- Predict ---------------------------------------\n",
    "            preds = model.predict(dtest)\n",
    "    \n",
    "            # preds_array = np.concatenate([preds_array, preds])\n",
    "            # id_array = np.concatenate([id_array, ids])\n",
    "            \n",
    "            # --- Save predictions -------------------------------\n",
    "            chunk_save_path = os.path.join(save_dir, f\"chunk_{i + 1}.npz\")\n",
    "            np.savez(chunk_save_path, ids=ids, preds=preds)\n",
    "    \n",
    "            \n",
    "            # --- Clean up ----------------------------------------\n",
    "            del chunk_df, features, labels, weights, dtest, ids, preds\n",
    "            sample_used = sample_count + chunk_df_count\n",
    "            sample_used_percentage = (sample_used / train_len) * 100\n",
    "            remaining_samples = train_len - sample_used\n",
    "            remaining_samples_percentage = (remaining_samples / train_len) * 100\n",
    "            pbar1.set_postfix_str(\n",
    "                f\"{sample_used} ({sample_used_percentage:.2f}%) samples used,\" \n",
    "                f\"{remaining_samples} ({remaining_samples_percentage:.2f}%) samples remaining\"\n",
    "            )\n",
    "            pbar1.update(1)\n",
    "            sample_count += chunk_df_count\n",
    "\n",
    "    return {'id': id_array, 'binds': preds_array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# incrementally_train()\n",
    "# spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_feat_tok_df_vectors = spark.read.format('parquet').load(chunks_path[1])\n",
    "# print(full_feat_tok_df_vectors.count())\n",
    "# full_feat_tok_df_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = joblib.load(\"Incrementally_train_XGB_ckpt/Final_ckpt_params.joblib\")['params']\n",
    "model = xgb.Booster(params=params)\n",
    "model.load_model(\"Incrementally_train_XGB_ckpt/Final_ckpt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/23m1521/.conda/envs/cuda_env2/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:38:52] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1733179637554/work/src/learner.cc:740: \n",
      "Parameters: { \"rate_drop\", \"skip_drop\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "                                                                                ples used,284908841 (96.50%) samples remaining]\r"
     ]
    }
   ],
   "source": [
    "full_preds = incrementally_inference(model, chunks_path, save_dir=\"Incrementally_Inference3_Preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0.152128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191.0</td>\n",
       "      <td>0.017978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418.0</td>\n",
       "      <td>0.010289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>541.0</td>\n",
       "      <td>0.069637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>558.0</td>\n",
       "      <td>0.064629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476723</th>\n",
       "      <td>295246296.0</td>\n",
       "      <td>0.370344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476724</th>\n",
       "      <td>295246331.0</td>\n",
       "      <td>0.045317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476725</th>\n",
       "      <td>295246454.0</td>\n",
       "      <td>0.609166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476726</th>\n",
       "      <td>295246584.0</td>\n",
       "      <td>0.015393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476727</th>\n",
       "      <td>295246670.0</td>\n",
       "      <td>0.005484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1476728 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     binds\n",
       "0               65.0  0.152128\n",
       "1              191.0  0.017978\n",
       "2              418.0  0.010289\n",
       "3              541.0  0.069637\n",
       "4              558.0  0.064629\n",
       "...              ...       ...\n",
       "1476723  295246296.0  0.370344\n",
       "1476724  295246331.0  0.045317\n",
       "1476725  295246454.0  0.609166\n",
       "1476726  295246584.0  0.015393\n",
       "1476727  295246670.0  0.005484\n",
       "\n",
       "[1476728 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_pred_df = pd.DataFrame(chunk_preds)\n",
    "chunk_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.152128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191</td>\n",
       "      <td>0.017978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418</td>\n",
       "      <td>0.010289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>541</td>\n",
       "      <td>0.069637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>558</td>\n",
       "      <td>0.064629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476723</th>\n",
       "      <td>295246296</td>\n",
       "      <td>0.370344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476724</th>\n",
       "      <td>295246331</td>\n",
       "      <td>0.045317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476725</th>\n",
       "      <td>295246454</td>\n",
       "      <td>0.609166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476726</th>\n",
       "      <td>295246584</td>\n",
       "      <td>0.015393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476727</th>\n",
       "      <td>295246670</td>\n",
       "      <td>0.005484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1476728 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     binds\n",
       "0               65  0.152128\n",
       "1              191  0.017978\n",
       "2              418  0.010289\n",
       "3              541  0.069637\n",
       "4              558  0.064629\n",
       "...            ...       ...\n",
       "1476723  295246296  0.370344\n",
       "1476724  295246331  0.045317\n",
       "1476725  295246454  0.609166\n",
       "1476726  295246584  0.015393\n",
       "1476727  295246670  0.005484\n",
       "\n",
       "[1476728 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_pred_df['id'] = chunk_pred_df['id'].astype(np.int64)\n",
    "chunk_pred_df = chunk_pred_df.sort_values(by=['id'])\n",
    "chunk_pred_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
